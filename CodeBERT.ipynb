{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(input):\n",
    "  source = {'CodeBERT_small' : {'tokenizer' : \"huggingface/CodeBERTa-small-v1\", 'model':\"huggingface/CodeBERTa-small-v1\"},\n",
    "            'CodeBERT' : {'tokenizer' : \"microsoft/codebert-base\", 'model':\"microsoft/codebert-base\"},\n",
    "            'GraphCodeBERT' : {'tokenizer' : \"microsoft/graphcodebert-base\", 'model':\"microsoft/graphcodebert-base\"},\n",
    "            'Fine-tuning_Example' : {'tokenizer':\"mrm8488/codebert-finetuned-clone-detection\", 'model':\"mrm8488/codebert-finetuned-clone-detection\"}\n",
    "  }\n",
    "  \n",
    "  if input in source.keys():\n",
    "    tokenizer = source[input]['tokenizer']\n",
    "    model = source[input]['model']\n",
    "  else:\n",
    "    print('Return None Obejects')\n",
    "    print('Models :', source.keys())\n",
    "    return None, None\n",
    "  \n",
    "  return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path, model_path = select_model('CodeBERT')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Code():\n",
    "    def __init__(self, path, test_size):\n",
    "        self.path = path\n",
    "        self.test_size = test_size\n",
    "            \n",
    "    def openFile(self):\n",
    "        df = pd.read_csv(self.path+'sample_train.csv')\n",
    "        df = df.sample(frac=1).reset_index(drop=True) # shuffle\n",
    "\n",
    "        # dataframe to ndarray\n",
    "        ndarray = pd.DataFrame.to_numpy(df)\n",
    "\n",
    "        # train/valid/test\n",
    "        train = ndarray[:-2*self.test_size]\n",
    "        valid = ndarray[-2*self.test_size:-self.test_size]\n",
    "        test = ndarray[-self.test_size:]\n",
    "\n",
    "        return train, valid, test\n",
    "\n",
    "class code_dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "\n",
    "        input_ids = []\n",
    "        attn_masks = []\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            tmp = tokenizer(data[i, 0], data[i,1], padding='max_length', truncation=True, return_tensors='pt')\n",
    "            input_ids.append(tmp['input_ids'])\n",
    "            attn_masks.append(tmp['attention_mask'])\n",
    "        \n",
    "        self.input_ids = torch.cat(input_ids, out=torch.Tensor(len(input_ids), 512)).type(torch.LongTensor)\n",
    "        self.attn_masks = torch.cat(attn_masks, out=torch.Tensor(len(attn_masks), 512)).type(torch.LongTensor)\n",
    "        self.labels =  torch.from_numpy(data[:, 2].astype(int))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.attn_masks[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def dataset_loader(path, test_size, batch_size):\n",
    "    code = Code(path, test_size=test_size)\n",
    "\n",
    "    train, valid, test = code.openFile()\n",
    "\n",
    "    train_dataset = code_dataset(train)\n",
    "    valid_dataset = code_dataset(valid)\n",
    "    test_dataset = code_dataset(test)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    valid_loader = DataLoader(dataset=valid_dataset,batch_size=batch_size,shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "def dataloader_testing():\n",
    "    # dataloader test\n",
    "    path = './data/'\n",
    "    train_loader, valid_loader, test_loader = dataset_loader(path, test_size=1000, batch_size=3) # batch_size = 3 (toy example)\n",
    "\n",
    "    data = next(iter(train_loader))\n",
    "\n",
    "    print('Batch Size :', len(data[0]))\n",
    "    print('input_ids: ', type(data[0]),'\\n', data[0])\n",
    "    print('attn_masks :', type(data[1]),'\\n', data[1])\n",
    "    print('labels :', type(data[2]),'\\n', data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, return_dict=False, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'\n",
    "train_loader, valid_loader, test_loader = dataset_loader(path, test_size=1000, batch_size=32) # batch size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "def cal_acc_and_loss(device, total_logits, labels_for_loss, pred, true):\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    num_batch = 0\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        pred_i = pred[i]\n",
    "        true_i = true[i]\n",
    "        \n",
    "        correct += np.sum(pred_i == true_i)\n",
    "        total_loss += loss_fn(total_logits[i].to('cpu'), labels_for_loss[i].to('cpu').float())\n",
    "\n",
    "        count += len(pred_i)\n",
    "        num_batch += 1\n",
    "    \n",
    "    acc = correct/count\n",
    "    loss = total_loss/num_batch\n",
    "\n",
    "    return correct, total_loss, count, num_batch, acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(model, device, data_loader):\n",
    "    total_logits, labels_for_loss, predictions , true_labels = [], [], [], []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        label_ids = b_labels\n",
    "\n",
    "        activation = nn.Softmax(dim=1)\n",
    "        logits = activation(logits)\n",
    "        \n",
    "        tmp_logits = torch.max(logits, dim=1)\n",
    "\n",
    "        logits = torch.abs(tmp_logits[0] + tmp_logits[1]-1)\n",
    "        pred = tmp_logits[1]\n",
    "\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "\n",
    "        total_logits.append(logits)\n",
    "        labels_for_loss.append(label_ids)\n",
    "        predictions.append(pred)\n",
    "        true_labels.append(label_ids.to('cpu').numpy())\n",
    "\n",
    "    correct, total_loss, count, num_batch, acc, loss = cal_acc_and_loss(device, total_logits, labels_for_loss, predictions, true_labels)\n",
    "\n",
    "    return acc, round(float(loss), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 over 4995\n",
      " Train Loss :  0.7106884717941284\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.7001\n",
      "     Valid Acc  : 0.501\n",
      "         model saved\n",
      "Step 10 over 4995\n",
      " Train Loss :  0.6984031796455383\n",
      "Step 20 over 4995\n",
      " Train Loss :  0.6996086955070495\n",
      "Step 30 over 4995\n",
      " Train Loss :  0.7033875524997711\n",
      "Step 40 over 4995\n",
      " Train Loss :  0.6937008678913117\n",
      "Step 50 over 4995\n",
      " Train Loss :  0.6958727180957794\n",
      "Step 60 over 4995\n",
      " Train Loss :  0.6898650825023651\n",
      "Step 70 over 4995\n",
      " Train Loss :  0.6945733904838562\n",
      "Step 80 over 4995\n",
      " Train Loss :  0.6885912537574768\n",
      "Step 90 over 4995\n",
      " Train Loss :  0.6973917782306671\n",
      "Step 100 over 4995\n",
      " Train Loss :  0.6997122704982758\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.6867\n",
      "     Valid Acc  : 0.527\n",
      "         model saved\n",
      "Step 110 over 4995\n",
      " Train Loss :  0.7044539213180542\n",
      "Step 120 over 4995\n",
      " Train Loss :  0.6943360686302185\n",
      "Step 130 over 4995\n",
      " Train Loss :  0.688327306509018\n",
      "Step 140 over 4995\n",
      " Train Loss :  0.6819635629653931\n",
      "Step 150 over 4995\n",
      " Train Loss :  0.7020406603813172\n",
      "Step 160 over 4995\n",
      " Train Loss :  0.6865079343318939\n",
      "Step 170 over 4995\n",
      " Train Loss :  0.7054219186306\n",
      "Step 180 over 4995\n",
      " Train Loss :  0.6902666866779328\n",
      "Step 190 over 4995\n",
      " Train Loss :  0.6627874314785004\n",
      "Step 200 over 4995\n",
      " Train Loss :  0.6898956060409546\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.678\n",
      "     Valid Acc  : 0.567\n",
      "         model saved\n",
      "Step 210 over 4995\n",
      " Train Loss :  0.654021966457367\n",
      "Step 220 over 4995\n",
      " Train Loss :  0.6780182480812073\n",
      "Step 230 over 4995\n",
      " Train Loss :  0.6657076835632324\n",
      "Step 240 over 4995\n",
      " Train Loss :  0.6929962515830994\n",
      "Step 250 over 4995\n",
      " Train Loss :  0.6803641021251678\n",
      "Step 260 over 4995\n",
      " Train Loss :  0.6848547339439393\n",
      "Step 270 over 4995\n",
      " Train Loss :  0.6573494970798492\n",
      "Step 280 over 4995\n",
      " Train Loss :  0.6909702479839325\n",
      "Step 290 over 4995\n",
      " Train Loss :  0.6750268757343292\n",
      "Step 300 over 4995\n",
      " Train Loss :  0.6518908977508545\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.6642\n",
      "     Valid Acc  : 0.594\n",
      "         model saved\n",
      "Step 310 over 4995\n",
      " Train Loss :  0.6738888144493103\n",
      "Step 320 over 4995\n",
      " Train Loss :  0.6920871198177337\n",
      "Step 330 over 4995\n",
      " Train Loss :  0.6537180721759797\n",
      "Step 340 over 4995\n",
      " Train Loss :  0.6425265610218048\n",
      "Step 350 over 4995\n",
      " Train Loss :  0.5901795506477356\n",
      "Step 360 over 4995\n",
      " Train Loss :  0.6416676938533783\n",
      "Step 370 over 4995\n",
      " Train Loss :  0.6128966391086579\n",
      "Step 380 over 4995\n",
      " Train Loss :  0.6449910283088685\n",
      "Step 390 over 4995\n",
      " Train Loss :  0.6003870010375977\n",
      "Step 400 over 4995\n",
      " Train Loss :  0.6405353546142578\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.5943\n",
      "     Valid Acc  : 0.694\n",
      "         model saved\n",
      "Step 410 over 4995\n",
      " Train Loss :  0.5983369022607803\n",
      "Step 420 over 4995\n",
      " Train Loss :  0.5370056867599488\n",
      "Step 430 over 4995\n",
      " Train Loss :  0.6511106967926026\n",
      "Step 440 over 4995\n",
      " Train Loss :  0.5477594494819641\n",
      "Step 450 over 4995\n",
      " Train Loss :  0.46170263886451723\n",
      "Step 460 over 4995\n",
      " Train Loss :  0.5679770141839982\n",
      "Step 470 over 4995\n",
      " Train Loss :  0.5263087540864945\n",
      "Step 480 over 4995\n",
      " Train Loss :  0.5279471188783645\n",
      "Step 490 over 4995\n",
      " Train Loss :  0.5079613208770752\n",
      "Step 500 over 4995\n",
      " Train Loss :  0.4319883644580841\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.4092\n",
      "     Valid Acc  : 0.814\n",
      "         model saved\n",
      "Step 510 over 4995\n",
      " Train Loss :  0.42975929379463196\n",
      "Step 520 over 4995\n",
      " Train Loss :  0.5122534215450287\n",
      "Step 530 over 4995\n",
      " Train Loss :  0.5428631246089936\n",
      "Step 540 over 4995\n",
      " Train Loss :  0.43382872343063356\n",
      "Step 550 over 4995\n",
      " Train Loss :  0.39974098801612856\n",
      "Step 560 over 4995\n",
      " Train Loss :  0.31398244947195053\n",
      "Step 570 over 4995\n",
      " Train Loss :  0.3430070780217648\n",
      "Step 580 over 4995\n",
      " Train Loss :  0.5126495137810707\n",
      "Step 590 over 4995\n",
      " Train Loss :  0.40587548166513443\n",
      "Step 600 over 4995\n",
      " Train Loss :  0.32305810749530794\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.3071\n",
      "     Valid Acc  : 0.869\n",
      "         model saved\n",
      "Step 610 over 4995\n",
      " Train Loss :  0.3567924708127975\n",
      "Step 620 over 4995\n",
      " Train Loss :  0.36270986050367354\n",
      "Step 630 over 4995\n",
      " Train Loss :  0.33742707818746565\n",
      "Step 640 over 4995\n",
      " Train Loss :  0.3513083517551422\n",
      "Step 650 over 4995\n",
      " Train Loss :  0.29567874073982237\n",
      "Step 660 over 4995\n",
      " Train Loss :  0.3772744409739971\n",
      "Step 670 over 4995\n",
      " Train Loss :  0.36438472718000414\n",
      "Step 680 over 4995\n",
      " Train Loss :  0.31484866291284563\n",
      "Step 690 over 4995\n",
      " Train Loss :  0.38090939819812775\n",
      "Step 700 over 4995\n",
      " Train Loss :  0.20880591496825218\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.2458\n",
      "     Valid Acc  : 0.897\n",
      "         model saved\n",
      "Step 710 over 4995\n",
      " Train Loss :  0.2508278384804726\n",
      "Step 720 over 4995\n",
      " Train Loss :  0.2724131219089031\n",
      "Step 730 over 4995\n",
      " Train Loss :  0.3064494527876377\n",
      "Step 740 over 4995\n",
      " Train Loss :  0.24262530952692032\n",
      "Step 750 over 4995\n",
      " Train Loss :  0.4002143397927284\n",
      "Step 760 over 4995\n",
      " Train Loss :  0.2727098621428013\n",
      "Step 770 over 4995\n",
      " Train Loss :  0.28136700540781023\n",
      "Step 780 over 4995\n",
      " Train Loss :  0.21658776327967644\n",
      "Step 790 over 4995\n",
      " Train Loss :  0.24494117498397827\n",
      "Step 800 over 4995\n",
      " Train Loss :  0.2924451928585768\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.2099\n",
      "     Valid Acc  : 0.917\n",
      "         model saved\n",
      "Step 810 over 4995\n",
      " Train Loss :  0.24632957950234413\n",
      "Step 820 over 4995\n",
      " Train Loss :  0.17774712294340134\n",
      "Step 830 over 4995\n",
      " Train Loss :  0.2470914475619793\n",
      "Step 840 over 4995\n",
      " Train Loss :  0.28167040795087817\n",
      "Step 850 over 4995\n",
      " Train Loss :  0.26359797939658164\n",
      "Step 860 over 4995\n",
      " Train Loss :  0.24526953250169753\n",
      "Step 870 over 4995\n",
      " Train Loss :  0.22083666399121285\n",
      "Step 880 over 4995\n",
      " Train Loss :  0.20524000450968743\n",
      "Step 890 over 4995\n",
      " Train Loss :  0.4002648189663887\n",
      "Step 900 over 4995\n",
      " Train Loss :  0.27400805801153183\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.2035\n",
      "     Valid Acc  : 0.922\n",
      "         model saved\n",
      "Step 910 over 4995\n",
      " Train Loss :  0.25643586218357084\n",
      "Step 920 over 4995\n",
      " Train Loss :  0.20070059821009636\n",
      "Step 930 over 4995\n",
      " Train Loss :  0.25277390629053115\n",
      "Step 940 over 4995\n",
      " Train Loss :  0.3292990315705538\n",
      "Step 950 over 4995\n",
      " Train Loss :  0.2346445620059967\n",
      "Step 960 over 4995\n",
      " Train Loss :  0.24511004909873008\n",
      "Step 970 over 4995\n",
      " Train Loss :  0.19847963973879815\n",
      "Step 980 over 4995\n",
      " Train Loss :  0.1529780812561512\n",
      "Step 990 over 4995\n",
      " Train Loss :  0.20573728233575822\n",
      "Step 1000 over 4995\n",
      " Train Loss :  0.2689280301332474\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.1727\n",
      "     Valid Acc  : 0.924\n",
      "         model saved\n",
      "Step 1010 over 4995\n",
      " Train Loss :  0.16482035219669341\n",
      "Step 1020 over 4995\n",
      " Train Loss :  0.23848709538578988\n",
      "Step 1030 over 4995\n",
      " Train Loss :  0.31035718098282816\n",
      "Step 1040 over 4995\n",
      " Train Loss :  0.29514969438314437\n",
      "Step 1050 over 4995\n",
      " Train Loss :  0.24189266860485076\n",
      "Step 1060 over 4995\n",
      " Train Loss :  0.13067784458398818\n",
      "Step 1070 over 4995\n",
      " Train Loss :  0.18602081201970577\n",
      "Step 1080 over 4995\n",
      " Train Loss :  0.15923422239720822\n",
      "Step 1090 over 4995\n",
      " Train Loss :  0.15162354297935962\n",
      "Step 1100 over 4995\n",
      " Train Loss :  0.12579532172530888\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.1468\n",
      "     Valid Acc  : 0.948\n",
      "         model saved\n",
      "Step 1110 over 4995\n",
      " Train Loss :  0.1859815025702119\n",
      "Step 1120 over 4995\n",
      " Train Loss :  0.16525873877108097\n",
      "Step 1130 over 4995\n",
      " Train Loss :  0.19487261530011893\n",
      "Step 1140 over 4995\n",
      " Train Loss :  0.17676934264600278\n",
      "Step 1150 over 4995\n",
      " Train Loss :  0.12614091970026492\n",
      "Step 1160 over 4995\n",
      " Train Loss :  0.1474439449608326\n",
      "Step 1170 over 4995\n",
      " Train Loss :  0.12269501630216836\n",
      "Step 1180 over 4995\n",
      " Train Loss :  0.13155670501291752\n",
      "Step 1190 over 4995\n",
      " Train Loss :  0.18875887468457223\n",
      "Step 1200 over 4995\n",
      " Train Loss :  0.17075145691633226\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.1327\n",
      "     Valid Acc  : 0.951\n",
      "         model saved\n",
      "Step 1210 over 4995\n",
      " Train Loss :  0.1466197682544589\n",
      "Step 1220 over 4995\n",
      " Train Loss :  0.15922400914132595\n",
      "Step 1230 over 4995\n",
      " Train Loss :  0.19582974389195443\n",
      "Step 1240 over 4995\n",
      " Train Loss :  0.19639758300036192\n",
      "Step 1250 over 4995\n",
      " Train Loss :  0.15188096575438975\n",
      "Step 1260 over 4995\n",
      " Train Loss :  0.17133004628121853\n",
      "Step 1270 over 4995\n",
      " Train Loss :  0.14113803617656231\n",
      "Step 1280 over 4995\n",
      " Train Loss :  0.23369999714195727\n",
      "Step 1290 over 4995\n",
      " Train Loss :  0.23356821443885564\n",
      "Step 1300 over 4995\n",
      " Train Loss :  0.17929858826100825\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.18\n",
      "     Valid Acc  : 0.925\n",
      "Step 1310 over 4995\n",
      " Train Loss :  0.17944038286805153\n",
      "Step 1320 over 4995\n",
      " Train Loss :  0.08130615521222354\n",
      "Step 1330 over 4995\n",
      " Train Loss :  0.21318809129297733\n",
      "Step 1340 over 4995\n",
      " Train Loss :  0.14742280300706626\n",
      "Step 1350 over 4995\n",
      " Train Loss :  0.18042991701513528\n",
      "Step 1360 over 4995\n",
      " Train Loss :  0.1193801298737526\n",
      "Step 1370 over 4995\n",
      " Train Loss :  0.1591586869210005\n",
      "Step 1380 over 4995\n",
      " Train Loss :  0.1364452216774225\n",
      "Step 1390 over 4995\n",
      " Train Loss :  0.11136568374931813\n",
      "Step 1400 over 4995\n",
      " Train Loss :  0.132387818582356\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.1338\n",
      "     Valid Acc  : 0.952\n",
      "Step 1410 over 4995\n",
      " Train Loss :  0.11900887209922076\n",
      "Step 1420 over 4995\n",
      " Train Loss :  0.1741935521364212\n",
      "Step 1430 over 4995\n",
      " Train Loss :  0.14715340379625558\n",
      "Step 1440 over 4995\n",
      " Train Loss :  0.1892074029892683\n",
      "Step 1450 over 4995\n",
      " Train Loss :  0.17902847845107317\n",
      "Step 1460 over 4995\n",
      " Train Loss :  0.16851027067750693\n",
      "Step 1470 over 4995\n",
      " Train Loss :  0.23323315307497977\n",
      "Step 1480 over 4995\n",
      " Train Loss :  0.11751940026879311\n",
      "Step 1490 over 4995\n",
      " Train Loss :  0.1875808797776699\n",
      "Step 1500 over 4995\n",
      " Train Loss :  0.10673837643116713\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.1449\n",
      "     Valid Acc  : 0.942\n",
      "Step 1510 over 4995\n",
      " Train Loss :  0.1596110506914556\n",
      "Step 1520 over 4995\n",
      " Train Loss :  0.20052525848150254\n",
      "Step 1530 over 4995\n",
      " Train Loss :  0.12134400345385074\n",
      "Step 1540 over 4995\n",
      " Train Loss :  0.15848220139741898\n",
      "Step 1550 over 4995\n",
      " Train Loss :  0.17311245538294315\n",
      "Step 1560 over 4995\n",
      " Train Loss :  0.11374467946588993\n",
      "Step 1570 over 4995\n",
      " Train Loss :  0.10321520287543536\n",
      "Step 1580 over 4995\n",
      " Train Loss :  0.15245388261973858\n",
      "Step 1590 over 4995\n",
      " Train Loss :  0.20249370262026786\n",
      "Step 1600 over 4995\n",
      " Train Loss :  0.136551165394485\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.1259\n",
      "     Valid Acc  : 0.953\n",
      "         model saved\n",
      "Step 1610 over 4995\n",
      " Train Loss :  0.11221007108688355\n",
      "Step 1620 over 4995\n",
      " Train Loss :  0.11406507100909949\n",
      "Step 1630 over 4995\n",
      " Train Loss :  0.15752364620566367\n",
      "Step 1640 over 4995\n",
      " Train Loss :  0.12890235371887684\n",
      "Step 1650 over 4995\n",
      " Train Loss :  0.17449365109205245\n",
      "Step 1660 over 4995\n",
      " Train Loss :  0.16843369398266078\n",
      "Step 1670 over 4995\n",
      " Train Loss :  0.062155204452574254\n",
      "Step 1680 over 4995\n",
      " Train Loss :  0.059996011201292276\n",
      "Step 1690 over 4995\n",
      " Train Loss :  0.19376164767891169\n",
      "Step 1700 over 4995\n",
      " Train Loss :  0.16263973340392113\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.1065\n",
      "     Valid Acc  : 0.963\n",
      "         model saved\n",
      "Step 1710 over 4995\n",
      " Train Loss :  0.05267992299050093\n",
      "Step 1720 over 4995\n",
      " Train Loss :  0.15868629086762667\n",
      "Step 1730 over 4995\n",
      " Train Loss :  0.12912394693121315\n",
      "Step 1740 over 4995\n",
      " Train Loss :  0.13704163171350955\n",
      "Step 1750 over 4995\n",
      " Train Loss :  0.10620156787335873\n",
      "Step 1760 over 4995\n",
      " Train Loss :  0.11532505117356777\n",
      "Step 1770 over 4995\n",
      " Train Loss :  0.07607407998293639\n",
      "Step 1780 over 4995\n",
      " Train Loss :  0.15578858368098736\n",
      "Step 1790 over 4995\n",
      " Train Loss :  0.09207661319524049\n",
      "Step 1800 over 4995\n",
      " Train Loss :  0.06406346056610346\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0907\n",
      "     Valid Acc  : 0.973\n",
      "         model saved\n",
      "Step 1810 over 4995\n",
      " Train Loss :  0.0745993297547102\n",
      "Step 1820 over 4995\n",
      " Train Loss :  0.08588653421029449\n",
      "Step 1830 over 4995\n",
      " Train Loss :  0.04583364240825176\n",
      "Step 1840 over 4995\n",
      " Train Loss :  0.11162538155913353\n",
      "Step 1850 over 4995\n",
      " Train Loss :  0.10514529263600707\n",
      "Step 1860 over 4995\n",
      " Train Loss :  0.09815750429406762\n",
      "Step 1870 over 4995\n",
      " Train Loss :  0.13906871546059846\n",
      "Step 1880 over 4995\n",
      " Train Loss :  0.13534542974084615\n",
      "Step 1890 over 4995\n",
      " Train Loss :  0.1657933384180069\n",
      "Step 1900 over 4995\n",
      " Train Loss :  0.12059523314237594\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.1267\n",
      "     Valid Acc  : 0.953\n",
      "Step 1910 over 4995\n",
      " Train Loss :  0.1094750076532364\n",
      "Step 1920 over 4995\n",
      " Train Loss :  0.09574040984734893\n",
      "Step 1930 over 4995\n",
      " Train Loss :  0.12011692067608237\n",
      "Step 1940 over 4995\n",
      " Train Loss :  0.21275229156017303\n",
      "Step 1950 over 4995\n",
      " Train Loss :  0.11712605692446232\n",
      "Step 1960 over 4995\n",
      " Train Loss :  0.12284837793558837\n",
      "Step 1970 over 4995\n",
      " Train Loss :  0.12421862557530403\n",
      "Step 1980 over 4995\n",
      " Train Loss :  0.10958007089793682\n",
      "Step 1990 over 4995\n",
      " Train Loss :  0.15119944382458925\n",
      "Step 2000 over 4995\n",
      " Train Loss :  0.16704965999815613\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0764\n",
      "     Valid Acc  : 0.976\n",
      "         model saved\n",
      "Step 2010 over 4995\n",
      " Train Loss :  0.11237615738064051\n",
      "Step 2020 over 4995\n",
      " Train Loss :  0.06540791802108288\n",
      "Step 2030 over 4995\n",
      " Train Loss :  0.047446231544017795\n",
      "Step 2040 over 4995\n",
      " Train Loss :  0.0819937204476446\n",
      "Step 2050 over 4995\n",
      " Train Loss :  0.05226691099815071\n",
      "Step 2060 over 4995\n",
      " Train Loss :  0.07630505990236998\n",
      "Step 2070 over 4995\n",
      " Train Loss :  0.12369451592676342\n",
      "Step 2080 over 4995\n",
      " Train Loss :  0.1002808442339301\n",
      "Step 2090 over 4995\n",
      " Train Loss :  0.03455083658918738\n",
      "Step 2100 over 4995\n",
      " Train Loss :  0.05200261753052473\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0851\n",
      "     Valid Acc  : 0.969\n",
      "Step 2110 over 4995\n",
      " Train Loss :  0.051990969898179175\n",
      "Step 2120 over 4995\n",
      " Train Loss :  0.036048987228423354\n",
      "Step 2130 over 4995\n",
      " Train Loss :  0.04646534621715546\n",
      "Step 2140 over 4995\n",
      " Train Loss :  0.19047240866348147\n",
      "Step 2150 over 4995\n",
      " Train Loss :  0.17304273741319776\n",
      "Step 2160 over 4995\n",
      " Train Loss :  0.08421046352013946\n",
      "Step 2170 over 4995\n",
      " Train Loss :  0.04122875304892659\n",
      "Step 2180 over 4995\n",
      " Train Loss :  0.09017676711082459\n",
      "Step 2190 over 4995\n",
      " Train Loss :  0.060700020659714936\n",
      "Step 2200 over 4995\n",
      " Train Loss :  0.06931412694975733\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0807\n",
      "     Valid Acc  : 0.973\n",
      "Step 2210 over 4995\n",
      " Train Loss :  0.07739353976212442\n",
      "Step 2220 over 4995\n",
      " Train Loss :  0.05794842806644738\n",
      "Step 2230 over 4995\n",
      " Train Loss :  0.02754942779429257\n",
      "Step 2240 over 4995\n",
      " Train Loss :  0.09843607200309634\n",
      "Step 2250 over 4995\n",
      " Train Loss :  0.05039193944539875\n",
      "Step 2260 over 4995\n",
      " Train Loss :  0.051983086438849566\n",
      "Step 2270 over 4995\n",
      " Train Loss :  0.06673809068743139\n",
      "Step 2280 over 4995\n",
      " Train Loss :  0.07266281330958009\n",
      "Step 2290 over 4995\n",
      " Train Loss :  0.1054830395616591\n",
      "Step 2300 over 4995\n",
      " Train Loss :  0.12441333187744022\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.1174\n",
      "     Valid Acc  : 0.959\n",
      "Step 2310 over 4995\n",
      " Train Loss :  0.1496745314449072\n",
      "Step 2320 over 4995\n",
      " Train Loss :  0.17954953871667384\n",
      "Step 2330 over 4995\n",
      " Train Loss :  0.141003191517666\n",
      "Step 2340 over 4995\n",
      " Train Loss :  0.11201873742975295\n",
      "Step 2350 over 4995\n",
      " Train Loss :  0.12946100179105996\n",
      "Step 2360 over 4995\n",
      " Train Loss :  0.07674508932977915\n",
      "Step 2370 over 4995\n",
      " Train Loss :  0.05479734912514687\n",
      "Step 2380 over 4995\n",
      " Train Loss :  0.035119714634492996\n",
      "Step 2390 over 4995\n",
      " Train Loss :  0.09493555491790176\n",
      "Step 2400 over 4995\n",
      " Train Loss :  0.04073411077260971\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0821\n",
      "     Valid Acc  : 0.971\n",
      "Step 2410 over 4995\n",
      " Train Loss :  0.11433329153805971\n",
      "Step 2420 over 4995\n",
      " Train Loss :  0.07726215142756701\n",
      "Step 2430 over 4995\n",
      " Train Loss :  0.06554855052381754\n",
      "Step 2440 over 4995\n",
      " Train Loss :  0.0853194803930819\n",
      "Step 2450 over 4995\n",
      " Train Loss :  0.04462580159306526\n",
      "Step 2460 over 4995\n",
      " Train Loss :  0.057526556751690806\n",
      "Step 2470 over 4995\n",
      " Train Loss :  0.06416989006102085\n",
      "Step 2480 over 4995\n",
      " Train Loss :  0.0516192864626646\n",
      "Step 2490 over 4995\n",
      " Train Loss :  0.06611597966402769\n",
      "Step 2500 over 4995\n",
      " Train Loss :  0.12707932163029909\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0603\n",
      "     Valid Acc  : 0.98\n",
      "         model saved\n",
      "Step 2510 over 4995\n",
      " Train Loss :  0.07480911435559393\n",
      "Step 2520 over 4995\n",
      " Train Loss :  0.03143294982146472\n",
      "Step 2530 over 4995\n",
      " Train Loss :  0.06983471987769008\n",
      "Step 2540 over 4995\n",
      " Train Loss :  0.10299844639375806\n",
      "Step 2550 over 4995\n",
      " Train Loss :  0.03625950338318944\n",
      "Step 2560 over 4995\n",
      " Train Loss :  0.08330613044090569\n",
      "Step 2570 over 4995\n",
      " Train Loss :  0.06749082757160067\n",
      "Step 2580 over 4995\n",
      " Train Loss :  0.11997854290530086\n",
      "Step 2590 over 4995\n",
      " Train Loss :  0.024542221706360577\n",
      "Step 2600 over 4995\n",
      " Train Loss :  0.0473955730907619\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0765\n",
      "     Valid Acc  : 0.972\n",
      "Step 2610 over 4995\n",
      " Train Loss :  0.06745488024316729\n",
      "Step 2620 over 4995\n",
      " Train Loss :  0.03390346276573837\n",
      "Step 2630 over 4995\n",
      " Train Loss :  0.05611838190816343\n",
      "Step 2640 over 4995\n",
      " Train Loss :  0.0631100029218942\n",
      "Step 2650 over 4995\n",
      " Train Loss :  0.15032022474333645\n",
      "Step 2660 over 4995\n",
      " Train Loss :  0.06068372093141079\n",
      "Step 2670 over 4995\n",
      " Train Loss :  0.06978625659830869\n",
      "Step 2680 over 4995\n",
      " Train Loss :  0.04801305877044797\n",
      "Step 2690 over 4995\n",
      " Train Loss :  0.02316759736277163\n",
      "Step 2700 over 4995\n",
      " Train Loss :  0.07562971480656415\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0827\n",
      "     Valid Acc  : 0.969\n",
      "Step 2710 over 4995\n",
      " Train Loss :  0.020236524962820114\n",
      "Step 2720 over 4995\n",
      " Train Loss :  0.04937596429372206\n",
      "Step 2730 over 4995\n",
      " Train Loss :  0.05278951246291399\n",
      "Step 2740 over 4995\n",
      " Train Loss :  0.09434864245122299\n",
      "Step 2750 over 4995\n",
      " Train Loss :  0.10277364542707801\n",
      "Step 2760 over 4995\n",
      " Train Loss :  0.07531103594228625\n",
      "Step 2770 over 4995\n",
      " Train Loss :  0.04544490189291537\n",
      "Step 2780 over 4995\n",
      " Train Loss :  0.06407558382488787\n",
      "Step 2790 over 4995\n",
      " Train Loss :  0.09461523247882724\n",
      "Step 2800 over 4995\n",
      " Train Loss :  0.06706294938921928\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.073\n",
      "     Valid Acc  : 0.977\n",
      "Step 2810 over 4995\n",
      " Train Loss :  0.1301997907459736\n",
      "Step 2820 over 4995\n",
      " Train Loss :  0.02519646470900625\n",
      "Step 2830 over 4995\n",
      " Train Loss :  0.0841361355734989\n",
      "Step 2840 over 4995\n",
      " Train Loss :  0.11750668454915285\n",
      "Step 2850 over 4995\n",
      " Train Loss :  0.08102473700419069\n",
      "Step 2860 over 4995\n",
      " Train Loss :  0.056266152579337356\n",
      "Step 2870 over 4995\n",
      " Train Loss :  0.03785934615880251\n",
      "Step 2880 over 4995\n",
      " Train Loss :  0.06886858977377415\n",
      "Step 2890 over 4995\n",
      " Train Loss :  0.05401648450642824\n",
      "Step 2900 over 4995\n",
      " Train Loss :  0.01758709833957255\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.1549\n",
      "     Valid Acc  : 0.951\n",
      "Step 2910 over 4995\n",
      " Train Loss :  0.06270100325345992\n",
      "Step 2920 over 4995\n",
      " Train Loss :  0.06422344886232167\n",
      "Step 2930 over 4995\n",
      " Train Loss :  0.04008712582290173\n",
      "Step 2940 over 4995\n",
      " Train Loss :  0.053155538509599864\n",
      "Step 2950 over 4995\n",
      " Train Loss :  0.038492899446282536\n",
      "Step 2960 over 4995\n",
      " Train Loss :  0.05645659170113504\n",
      "Step 2970 over 4995\n",
      " Train Loss :  0.025512188021093608\n",
      "Step 2980 over 4995\n",
      " Train Loss :  0.09330328744836151\n",
      "Step 2990 over 4995\n",
      " Train Loss :  0.10463442637119443\n",
      "Step 3000 over 4995\n",
      " Train Loss :  0.06863831896334886\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0674\n",
      "     Valid Acc  : 0.978\n",
      "Step 3010 over 4995\n",
      " Train Loss :  0.04634033520705998\n",
      "Step 3020 over 4995\n",
      " Train Loss :  0.05104539208114147\n",
      "Step 3030 over 4995\n",
      " Train Loss :  0.035426897648721936\n",
      "Step 3040 over 4995\n",
      " Train Loss :  0.04538284195587039\n",
      "Step 3050 over 4995\n",
      " Train Loss :  0.029949569329619407\n",
      "Step 3060 over 4995\n",
      " Train Loss :  0.042391282424796374\n",
      "Step 3070 over 4995\n",
      " Train Loss :  0.03245656881481409\n",
      "Step 3080 over 4995\n",
      " Train Loss :  0.11341185173951089\n",
      "Step 3090 over 4995\n",
      " Train Loss :  0.06434632586315274\n",
      "Step 3100 over 4995\n",
      " Train Loss :  0.09716685828752816\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.1386\n",
      "     Valid Acc  : 0.955\n",
      "Step 3110 over 4995\n",
      " Train Loss :  0.024089808901771904\n",
      "Step 3120 over 4995\n",
      " Train Loss :  0.030621567799244076\n",
      "Step 3130 over 4995\n",
      " Train Loss :  0.02744225664064288\n",
      "Step 3140 over 4995\n",
      " Train Loss :  0.024764540721662343\n",
      "Step 3150 over 4995\n",
      " Train Loss :  0.052781210956163706\n",
      "Step 3160 over 4995\n",
      " Train Loss :  0.04194331336766481\n",
      "Step 3170 over 4995\n",
      " Train Loss :  0.038291680393740535\n",
      "Step 3180 over 4995\n",
      " Train Loss :  0.007752612902550027\n",
      "Step 3190 over 4995\n",
      " Train Loss :  0.036616962705738845\n",
      "Step 3200 over 4995\n",
      " Train Loss :  0.010351666738279164\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0749\n",
      "     Valid Acc  : 0.979\n",
      "Step 3210 over 4995\n",
      " Train Loss :  0.017424186813877897\n",
      "Step 3220 over 4995\n",
      " Train Loss :  0.0218932609714102\n",
      "Step 3230 over 4995\n",
      " Train Loss :  0.04642609608708881\n",
      "Step 3240 over 4995\n",
      " Train Loss :  0.14434673096984624\n",
      "Step 3250 over 4995\n",
      " Train Loss :  0.11344621712341904\n",
      "Step 3260 over 4995\n",
      " Train Loss :  0.038106217584572735\n",
      "Step 3270 over 4995\n",
      " Train Loss :  0.041157692205160856\n",
      "Step 3280 over 4995\n",
      " Train Loss :  0.046703617181628944\n",
      "Step 3290 over 4995\n",
      " Train Loss :  0.040779782272875306\n",
      "Step 3300 over 4995\n",
      " Train Loss :  0.011070850072428584\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.1161\n",
      "     Valid Acc  : 0.964\n",
      "Step 3310 over 4995\n",
      " Train Loss :  0.040584682719781995\n",
      "Step 3320 over 4995\n",
      " Train Loss :  0.025439446372911335\n",
      "Step 3330 over 4995\n",
      " Train Loss :  0.040495588118210434\n",
      "Step 3340 over 4995\n",
      " Train Loss :  0.033356099820230155\n",
      "Step 3350 over 4995\n",
      " Train Loss :  0.07159416694194079\n",
      "Step 3360 over 4995\n",
      " Train Loss :  0.04616559366695583\n",
      "Step 3370 over 4995\n",
      " Train Loss :  0.02136144010582939\n",
      "Step 3380 over 4995\n",
      " Train Loss :  0.03345639274921268\n",
      "Step 3390 over 4995\n",
      " Train Loss :  0.022149113996420056\n",
      "Step 3400 over 4995\n",
      " Train Loss :  0.01311761699616909\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0824\n",
      "     Valid Acc  : 0.977\n",
      "Step 3410 over 4995\n",
      " Train Loss :  0.00806790241622366\n",
      "Step 3420 over 4995\n",
      " Train Loss :  0.022180335462326183\n",
      "Step 3430 over 4995\n",
      " Train Loss :  0.07649477470549755\n",
      "Step 3440 over 4995\n",
      " Train Loss :  0.05114708805922419\n",
      "Step 3450 over 4995\n",
      " Train Loss :  0.07947014076635242\n",
      "Step 3460 over 4995\n",
      " Train Loss :  0.09349981052801012\n",
      "Step 3470 over 4995\n",
      " Train Loss :  0.0857046495191753\n",
      "Step 3480 over 4995\n",
      " Train Loss :  0.050814292021095754\n",
      "Step 3490 over 4995\n",
      " Train Loss :  0.048339548252988605\n",
      "Step 3500 over 4995\n",
      " Train Loss :  0.023045630124397577\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0638\n",
      "     Valid Acc  : 0.978\n",
      "Step 3510 over 4995\n",
      " Train Loss :  0.034038212755694984\n",
      "Step 3520 over 4995\n",
      " Train Loss :  0.0444823078927584\n",
      "Step 3530 over 4995\n",
      " Train Loss :  0.02428925468120724\n",
      "Step 3540 over 4995\n",
      " Train Loss :  0.06278867985820398\n",
      "Step 3550 over 4995\n",
      " Train Loss :  0.020683428714983167\n",
      "Step 3560 over 4995\n",
      " Train Loss :  0.04005603305995464\n",
      "Step 3570 over 4995\n",
      " Train Loss :  0.04313113614916801\n",
      "Step 3580 over 4995\n",
      " Train Loss :  0.03439447437413037\n",
      "Step 3590 over 4995\n",
      " Train Loss :  0.06071171110961586\n",
      "Step 3600 over 4995\n",
      " Train Loss :  0.09215132389217615\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0919\n",
      "     Valid Acc  : 0.971\n",
      "Step 3610 over 4995\n",
      " Train Loss :  0.06971694007515908\n",
      "Step 3620 over 4995\n",
      " Train Loss :  0.05683264094404876\n",
      "Step 3630 over 4995\n",
      " Train Loss :  0.033784588344860825\n",
      "Step 3640 over 4995\n",
      " Train Loss :  0.11214816477149725\n",
      "Step 3650 over 4995\n",
      " Train Loss :  0.038494132552295925\n",
      "Step 3660 over 4995\n",
      " Train Loss :  0.05219346135854721\n",
      "Step 3670 over 4995\n",
      " Train Loss :  0.03582475851289928\n",
      "Step 3680 over 4995\n",
      " Train Loss :  0.025222228351049127\n",
      "Step 3690 over 4995\n",
      " Train Loss :  0.07757067709462717\n",
      "Step 3700 over 4995\n",
      " Train Loss :  0.028601076966151596\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0801\n",
      "     Valid Acc  : 0.975\n",
      "Step 3710 over 4995\n",
      " Train Loss :  0.06969717891188339\n",
      "Step 3720 over 4995\n",
      " Train Loss :  0.00924429097212851\n",
      "Step 3730 over 4995\n",
      " Train Loss :  0.05112501119729131\n",
      "Step 3740 over 4995\n",
      " Train Loss :  0.03388963465113193\n",
      "Step 3750 over 4995\n",
      " Train Loss :  0.06187864202074707\n",
      "Step 3760 over 4995\n",
      " Train Loss :  0.10989297556225211\n",
      "Step 3770 over 4995\n",
      " Train Loss :  0.08248139249626547\n",
      "Step 3780 over 4995\n",
      " Train Loss :  0.052495379094034435\n",
      "Step 3790 over 4995\n",
      " Train Loss :  0.06320080284494907\n",
      "Step 3800 over 4995\n",
      " Train Loss :  0.053552500810474156\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0826\n",
      "     Valid Acc  : 0.973\n",
      "Step 3810 over 4995\n",
      " Train Loss :  0.0437685614451766\n",
      "Step 3820 over 4995\n",
      " Train Loss :  0.08310271052177995\n",
      "Step 3830 over 4995\n",
      " Train Loss :  0.09262957274913788\n",
      "Step 3840 over 4995\n",
      " Train Loss :  0.041227696696296334\n",
      "Step 3850 over 4995\n",
      " Train Loss :  0.030130012473091484\n",
      "Step 3860 over 4995\n",
      " Train Loss :  0.03140011818613857\n",
      "Step 3870 over 4995\n",
      " Train Loss :  0.015415446693077683\n",
      "Step 3880 over 4995\n",
      " Train Loss :  0.01604105649748817\n",
      "Step 3890 over 4995\n",
      " Train Loss :  0.03459992106072605\n",
      "Step 3900 over 4995\n",
      " Train Loss :  0.010251321451505646\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0689\n",
      "     Valid Acc  : 0.981\n",
      "Step 3910 over 4995\n",
      " Train Loss :  0.01740257540368475\n",
      "Step 3920 over 4995\n",
      " Train Loss :  0.02611797418212518\n",
      "Step 3930 over 4995\n",
      " Train Loss :  0.027472850983031094\n",
      "Step 3940 over 4995\n",
      " Train Loss :  0.018511719396337868\n",
      "Step 3950 over 4995\n",
      " Train Loss :  0.013625005667563528\n",
      "Step 3960 over 4995\n",
      " Train Loss :  0.014003599580610171\n",
      "Step 3970 over 4995\n",
      " Train Loss :  0.02502467406447977\n",
      "Step 3980 over 4995\n",
      " Train Loss :  0.03903007530607283\n",
      "Step 3990 over 4995\n",
      " Train Loss :  0.013852988230064511\n",
      "Step 4000 over 4995\n",
      " Train Loss :  0.013021065469365567\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0698\n",
      "     Valid Acc  : 0.98\n",
      "Step 4010 over 4995\n",
      " Train Loss :  0.01567067831638269\n",
      "Step 4020 over 4995\n",
      " Train Loss :  0.00604378369753249\n",
      "Step 4030 over 4995\n",
      " Train Loss :  0.008448742731707171\n",
      "Step 4040 over 4995\n",
      " Train Loss :  0.029356965608894826\n",
      "Step 4050 over 4995\n",
      " Train Loss :  0.0339489663252607\n",
      "Step 4060 over 4995\n",
      " Train Loss :  0.021175206857151352\n",
      "Step 4070 over 4995\n",
      " Train Loss :  0.039726131083443764\n",
      "Step 4080 over 4995\n",
      " Train Loss :  0.060419331642333415\n",
      "Step 4090 over 4995\n",
      " Train Loss :  0.05177874208893627\n",
      "Step 4100 over 4995\n",
      " Train Loss :  0.013023222208721564\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0687\n",
      "     Valid Acc  : 0.978\n",
      "Step 4110 over 4995\n",
      " Train Loss :  0.009900044940877706\n",
      "Step 4120 over 4995\n",
      " Train Loss :  0.034892245350056326\n",
      "Step 4130 over 4995\n",
      " Train Loss :  0.042181880393764005\n",
      "Step 4140 over 4995\n",
      " Train Loss :  0.007243504468351603\n",
      "Step 4150 over 4995\n",
      " Train Loss :  0.027378968946868552\n",
      "Step 4160 over 4995\n",
      " Train Loss :  0.042336655023973435\n",
      "Step 4170 over 4995\n",
      " Train Loss :  0.017853013874264433\n",
      "Step 4180 over 4995\n",
      " Train Loss :  0.029746608767891303\n",
      "Step 4190 over 4995\n",
      " Train Loss :  0.030960804887581617\n",
      "Step 4200 over 4995\n",
      " Train Loss :  0.041089268767973405\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0758\n",
      "     Valid Acc  : 0.978\n",
      "Step 4210 over 4995\n",
      " Train Loss :  0.034929704930982554\n",
      "Step 4220 over 4995\n",
      " Train Loss :  0.06307046007132158\n",
      "Step 4230 over 4995\n",
      " Train Loss :  0.017290102096740156\n",
      "Step 4240 over 4995\n",
      " Train Loss :  0.005968880234286189\n",
      "Step 4250 over 4995\n",
      " Train Loss :  0.029399839357938617\n",
      "Step 4260 over 4995\n",
      " Train Loss :  0.02186220803996548\n",
      "Step 4270 over 4995\n",
      " Train Loss :  0.04378634627792053\n",
      "Step 4280 over 4995\n",
      " Train Loss :  0.0024518161022569983\n",
      "Step 4290 over 4995\n",
      " Train Loss :  0.05200048533733934\n",
      "Step 4300 over 4995\n",
      " Train Loss :  0.04692409077833872\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0678\n",
      "     Valid Acc  : 0.977\n",
      "Step 4310 over 4995\n",
      " Train Loss :  0.007041269162436947\n",
      "Step 4320 over 4995\n",
      " Train Loss :  0.017954200506210327\n",
      "Step 4330 over 4995\n",
      " Train Loss :  0.08272647480480373\n",
      "Step 4340 over 4995\n",
      " Train Loss :  0.022439300711266697\n",
      "Step 4350 over 4995\n",
      " Train Loss :  0.010672238655388355\n",
      "Step 4360 over 4995\n",
      " Train Loss :  0.03540541419060901\n",
      "Step 4370 over 4995\n",
      " Train Loss :  0.00732009788043797\n",
      "Step 4380 over 4995\n",
      " Train Loss :  0.028044881852110848\n",
      "Step 4390 over 4995\n",
      " Train Loss :  0.06212451565079391\n",
      "Step 4400 over 4995\n",
      " Train Loss :  0.06554857886512763\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0821\n",
      "     Valid Acc  : 0.979\n",
      "Step 4410 over 4995\n",
      " Train Loss :  0.018358956457814202\n",
      "Step 4420 over 4995\n",
      " Train Loss :  0.02300205366918817\n",
      "Step 4430 over 4995\n",
      " Train Loss :  0.02822190395090729\n",
      "Step 4440 over 4995\n",
      " Train Loss :  0.010714684362756088\n",
      "Step 4450 over 4995\n",
      " Train Loss :  0.022580103971995412\n",
      "Step 4460 over 4995\n",
      " Train Loss :  0.01330021199537441\n",
      "Step 4470 over 4995\n",
      " Train Loss :  0.019242280308390036\n",
      "Step 4480 over 4995\n",
      " Train Loss :  0.04308649910381064\n",
      "Step 4490 over 4995\n",
      " Train Loss :  0.009800112975062803\n",
      "Step 4500 over 4995\n",
      " Train Loss :  0.007447189331287518\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0596\n",
      "     Valid Acc  : 0.984\n",
      "         model saved\n",
      "Step 4510 over 4995\n",
      " Train Loss :  0.01214884216315113\n",
      "Step 4520 over 4995\n",
      " Train Loss :  0.01040077030484099\n",
      "Step 4530 over 4995\n",
      " Train Loss :  0.010515536341699772\n",
      "Step 4540 over 4995\n",
      " Train Loss :  0.006021181264077313\n",
      "Step 4550 over 4995\n",
      " Train Loss :  0.007306915533263236\n",
      "Step 4560 over 4995\n",
      " Train Loss :  0.09197215122985654\n",
      "Step 4570 over 4995\n",
      " Train Loss :  0.01630070246174\n",
      "Step 4580 over 4995\n",
      " Train Loss :  0.02565636406070553\n",
      "Step 4590 over 4995\n",
      " Train Loss :  0.013837945653358474\n",
      "Step 4600 over 4995\n",
      " Train Loss :  0.06185750676086173\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.068\n",
      "     Valid Acc  : 0.987\n",
      "Step 4610 over 4995\n",
      " Train Loss :  0.015296505019068717\n",
      "Step 4620 over 4995\n",
      " Train Loss :  0.03986927663499955\n",
      "Step 4630 over 4995\n",
      " Train Loss :  0.06231078284326941\n",
      "Step 4640 over 4995\n",
      " Train Loss :  0.019247889047255738\n",
      "Step 4650 over 4995\n",
      " Train Loss :  0.09522551103727891\n",
      "Step 4660 over 4995\n",
      " Train Loss :  0.05956045214552432\n",
      "Step 4670 over 4995\n",
      " Train Loss :  0.014680376660544425\n",
      "Step 4680 over 4995\n",
      " Train Loss :  0.06639154243748635\n",
      "Step 4690 over 4995\n",
      " Train Loss :  0.03428468172205612\n",
      "Step 4700 over 4995\n",
      " Train Loss :  0.012164421193301678\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0566\n",
      "     Valid Acc  : 0.982\n",
      "         model saved\n",
      "Step 4710 over 4995\n",
      " Train Loss :  0.007177248480729759\n",
      "Step 4720 over 4995\n",
      " Train Loss :  0.010014820867218078\n",
      "Step 4730 over 4995\n",
      " Train Loss :  0.02580253966152668\n",
      "Step 4740 over 4995\n",
      " Train Loss :  0.013558428653050214\n",
      "Step 4750 over 4995\n",
      " Train Loss :  0.007037522096652538\n",
      "Step 4760 over 4995\n",
      " Train Loss :  0.037712610355811194\n",
      "Step 4770 over 4995\n",
      " Train Loss :  0.009983013826422393\n",
      "Step 4780 over 4995\n",
      " Train Loss :  0.015991930940072053\n",
      "Step 4790 over 4995\n",
      " Train Loss :  0.013006484610377811\n",
      "Step 4800 over 4995\n",
      " Train Loss :  0.006230551208136603\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.0849\n",
      "     Valid Acc  : 0.977\n",
      "Step 4810 over 4995\n",
      " Train Loss :  0.04990513774100691\n",
      "Step 4820 over 4995\n",
      " Train Loss :  0.02645754946861416\n",
      "Step 4830 over 4995\n",
      " Train Loss :  0.0350074686226435\n",
      "Step 4840 over 4995\n",
      " Train Loss :  0.11757914768531919\n",
      "Step 4850 over 4995\n",
      " Train Loss :  0.05086064592469484\n",
      "Step 4860 over 4995\n",
      " Train Loss :  0.03350705874618143\n",
      "Step 4870 over 4995\n",
      " Train Loss :  0.0183831850416027\n",
      "Step 4880 over 4995\n",
      " Train Loss :  0.011232148006092757\n",
      "Step 4890 over 4995\n",
      " Train Loss :  0.00906103557208553\n",
      "Step 4900 over 4995\n",
      " Train Loss :  0.016563957068137826\n",
      "     Report Performance on Validation Dataset...\n",
      "     Valid Loss : 0.082\n",
      "     Valid Acc  : 0.981\n",
      "Step 4910 over 4995\n",
      " Train Loss :  0.008673258789349348\n",
      "Step 4920 over 4995\n",
      " Train Loss :  0.04208493417827412\n",
      "Step 4930 over 4995\n",
      " Train Loss :  0.02606091877678409\n",
      "Step 4940 over 4995\n",
      " Train Loss :  0.011085648613516241\n",
      "Step 4950 over 4995\n",
      " Train Loss :  0.01628986721043475\n",
      "Step 4960 over 4995\n",
      " Train Loss :  0.035917762044118715\n",
      "Step 4970 over 4995\n",
      " Train Loss :  0.022592248395085335\n",
      "Step 4980 over 4995\n",
      " Train Loss :  0.0030980711919255553\n",
      "Step 4990 over 4995\n",
      " Train Loss :  0.01711088073789142\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 8e-6, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "num_epochs = 5\n",
    "step_count = 0\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "valid_loss_min = 1.0\n",
    "\n",
    "loss_over_10_steps = []\n",
    "\n",
    "model_path = './models/'\n",
    "saved_at_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (input_ids, attn_masks, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attn_masks = attn_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attn_masks, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_over_10_steps.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if step_count%10 == 0:\n",
    "            print('Step {} over {}'.format(step_count, total_steps))\n",
    "            print(' Train Loss : ', np.mean(loss_over_10_steps))\n",
    "            train_loss.append(np.mean(loss_over_10_steps))\n",
    "            loss_over_10_steps = []\n",
    "        \n",
    "        # Report Validation Performance\n",
    "        model.eval()\n",
    "        if step_count%100 == 0:\n",
    "            val_acc, val_loss = performance(model, device, valid_loader)\n",
    "            print('     Report Performance on Validation Dataset...')\n",
    "            print('     Valid Loss :', val_loss)\n",
    "            print('     Valid Acc  :', val_acc)\n",
    "            valid_acc.append(val_acc)\n",
    "            valid_loss.append(val_loss)\n",
    "\n",
    "            # save model\n",
    "            if valid_loss_min > val_loss:\n",
    "                valid_loss_min = val_loss\n",
    "                print('         model saved')\n",
    "                torch.save(model, model_path + 'model.pt')  # 전체 모델 저장\n",
    "                torch.save({\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "                }, model_path + 'all.tar')  # 여러 가지 값 저장, 학습 중 진행 상황 저장을 위해 epoch, loss 값 등 일반 scalar값 저장 가능\n",
    "                saved_at_step = step_count\n",
    "        \n",
    "        step_count = step_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f55ede7bdc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_x_axis = [10*i for i in range(len(train_loss))]\n",
    "valid_x_axis = [100*i for i in range(len(valid_loss))]\n",
    "\n",
    "plt.axhline(y=0.69, label='Random', color='red')\n",
    "plt.plot(train_x_axis, train_loss, label='Train Loss')\n",
    "plt.plot(valid_x_axis, valid_loss, marker='o', ms=5, color='black', label='Valid Loss')\n",
    "\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f55c15b3a30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(valid_x_axis, valid_acc, marker='o', ms=5, label='Validation Accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('step')\n",
    "plt.ylim(0.4,1)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for test dataset...\n",
      "Test ACC :  0.981\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for test dataset...')\n",
    "\n",
    "# load model\n",
    "model_path = './models/'\n",
    "model = torch.load(model_path + 'model.pt')  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
    "checkpoint = torch.load(model_path + 'all.tar')   # dict 불러오기\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_acc, _ = performance(model, device, test_loader)\n",
    "\n",
    "print('Test ACC : ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Code():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "            \n",
    "    def openFile(self):\n",
    "        df = pd.read_csv(self.path+'test.csv')\n",
    "\n",
    "        # dataframe to ndarray\n",
    "        data = pd.DataFrame.to_numpy(df)\n",
    "\n",
    "        return data\n",
    "\n",
    "class test_code_dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "\n",
    "        input_ids = []\n",
    "        attn_masks = []\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            tmp = tokenizer(data[i, 1], data[i,2], padding='max_length', truncation=True, return_tensors='pt')\n",
    "            input_ids.append(tmp['input_ids'])\n",
    "            attn_masks.append(tmp['attention_mask'])\n",
    "        \n",
    "        self.input_ids = torch.cat(input_ids, out=torch.Tensor(len(input_ids), 512)).type(torch.LongTensor)\n",
    "        self.attn_masks = torch.cat(attn_masks, out=torch.Tensor(len(attn_masks), 512)).type(torch.LongTensor)\n",
    "        self.pair_ids = torch.from_numpy(data[:, 0].astype(int))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.attn_masks[index], self.pair_ids[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_ids)\n",
    "\n",
    "def test_dataset_loader(path, batch_size):\n",
    "    code = Test_Code(path)\n",
    "\n",
    "    data = code.openFile()\n",
    "\n",
    "    dataset = test_code_dataset(data)\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "def test_dataloader_testing():\n",
    "    # dataloader test\n",
    "    path = './'\n",
    "    data_loader = test_dataset_loader(path=path, batch_size=16) # batch_size = 10 (toy example)\n",
    "\n",
    "    data = next(iter(train_loader))\n",
    "\n",
    "    print('Batch Size :', len(data[0]))\n",
    "    print('input_ids: ', type(data[0]),'\\n', data[0])\n",
    "    print('attn_masks :', type(data[1]),'\\n', data[1])\n",
    "    print('pair_ids :', type(data[2]), '\\n', data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'\n",
    "data_loader = test_dataset_loader(path=path, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 10 over 11232\n",
      "batch 20 over 11232\n",
      "batch 30 over 11232\n",
      "batch 40 over 11232\n",
      "batch 50 over 11232\n",
      "batch 60 over 11232\n",
      "batch 70 over 11232\n",
      "batch 80 over 11232\n",
      "batch 90 over 11232\n",
      "batch 100 over 11232\n",
      "batch 110 over 11232\n",
      "batch 120 over 11232\n",
      "batch 130 over 11232\n",
      "batch 140 over 11232\n",
      "batch 150 over 11232\n",
      "batch 160 over 11232\n",
      "batch 170 over 11232\n",
      "batch 180 over 11232\n",
      "batch 190 over 11232\n",
      "batch 200 over 11232\n",
      "batch 210 over 11232\n",
      "batch 220 over 11232\n",
      "batch 230 over 11232\n",
      "batch 240 over 11232\n",
      "batch 250 over 11232\n",
      "batch 260 over 11232\n",
      "batch 270 over 11232\n",
      "batch 280 over 11232\n",
      "batch 290 over 11232\n",
      "batch 300 over 11232\n",
      "batch 310 over 11232\n",
      "batch 320 over 11232\n",
      "batch 330 over 11232\n",
      "batch 340 over 11232\n",
      "batch 350 over 11232\n",
      "batch 360 over 11232\n",
      "batch 370 over 11232\n",
      "batch 380 over 11232\n",
      "batch 390 over 11232\n",
      "batch 400 over 11232\n",
      "batch 410 over 11232\n",
      "batch 420 over 11232\n",
      "batch 430 over 11232\n",
      "batch 440 over 11232\n",
      "batch 450 over 11232\n",
      "batch 460 over 11232\n",
      "batch 470 over 11232\n",
      "batch 480 over 11232\n",
      "batch 490 over 11232\n",
      "batch 500 over 11232\n",
      "batch 510 over 11232\n",
      "batch 520 over 11232\n",
      "batch 530 over 11232\n",
      "batch 540 over 11232\n",
      "batch 550 over 11232\n",
      "batch 560 over 11232\n",
      "batch 570 over 11232\n",
      "batch 580 over 11232\n",
      "batch 590 over 11232\n",
      "batch 600 over 11232\n",
      "batch 610 over 11232\n",
      "batch 620 over 11232\n",
      "batch 630 over 11232\n",
      "batch 640 over 11232\n",
      "batch 650 over 11232\n",
      "batch 660 over 11232\n",
      "batch 670 over 11232\n",
      "batch 680 over 11232\n",
      "batch 690 over 11232\n",
      "batch 700 over 11232\n",
      "batch 710 over 11232\n",
      "batch 720 over 11232\n",
      "batch 730 over 11232\n",
      "batch 740 over 11232\n",
      "batch 750 over 11232\n",
      "batch 760 over 11232\n",
      "batch 770 over 11232\n",
      "batch 780 over 11232\n",
      "batch 790 over 11232\n",
      "batch 800 over 11232\n",
      "batch 810 over 11232\n",
      "batch 820 over 11232\n",
      "batch 830 over 11232\n",
      "batch 840 over 11232\n",
      "batch 850 over 11232\n",
      "batch 860 over 11232\n",
      "batch 870 over 11232\n",
      "batch 880 over 11232\n",
      "batch 890 over 11232\n",
      "batch 900 over 11232\n",
      "batch 910 over 11232\n",
      "batch 920 over 11232\n",
      "batch 930 over 11232\n",
      "batch 940 over 11232\n",
      "batch 950 over 11232\n",
      "batch 960 over 11232\n",
      "batch 970 over 11232\n",
      "batch 980 over 11232\n",
      "batch 990 over 11232\n",
      "batch 1000 over 11232\n",
      "batch 1010 over 11232\n",
      "batch 1020 over 11232\n",
      "batch 1030 over 11232\n",
      "batch 1040 over 11232\n",
      "batch 1050 over 11232\n",
      "batch 1060 over 11232\n",
      "batch 1070 over 11232\n",
      "batch 1080 over 11232\n",
      "batch 1090 over 11232\n",
      "batch 1100 over 11232\n",
      "batch 1110 over 11232\n",
      "batch 1120 over 11232\n",
      "batch 1130 over 11232\n",
      "batch 1140 over 11232\n",
      "batch 1150 over 11232\n",
      "batch 1160 over 11232\n",
      "batch 1170 over 11232\n",
      "batch 1180 over 11232\n",
      "batch 1190 over 11232\n",
      "batch 1200 over 11232\n",
      "batch 1210 over 11232\n",
      "batch 1220 over 11232\n",
      "batch 1230 over 11232\n",
      "batch 1240 over 11232\n",
      "batch 1250 over 11232\n",
      "batch 1260 over 11232\n",
      "batch 1270 over 11232\n",
      "batch 1280 over 11232\n",
      "batch 1290 over 11232\n",
      "batch 1300 over 11232\n",
      "batch 1310 over 11232\n",
      "batch 1320 over 11232\n",
      "batch 1330 over 11232\n",
      "batch 1340 over 11232\n",
      "batch 1350 over 11232\n",
      "batch 1360 over 11232\n",
      "batch 1370 over 11232\n",
      "batch 1380 over 11232\n",
      "batch 1390 over 11232\n",
      "batch 1400 over 11232\n",
      "batch 1410 over 11232\n",
      "batch 1420 over 11232\n",
      "batch 1430 over 11232\n",
      "batch 1440 over 11232\n",
      "batch 1450 over 11232\n",
      "batch 1460 over 11232\n",
      "batch 1470 over 11232\n",
      "batch 1480 over 11232\n",
      "batch 1490 over 11232\n",
      "batch 1500 over 11232\n",
      "batch 1510 over 11232\n",
      "batch 1520 over 11232\n",
      "batch 1530 over 11232\n",
      "batch 1540 over 11232\n",
      "batch 1550 over 11232\n",
      "batch 1560 over 11232\n",
      "batch 1570 over 11232\n",
      "batch 1580 over 11232\n",
      "batch 1590 over 11232\n",
      "batch 1600 over 11232\n",
      "batch 1610 over 11232\n",
      "batch 1620 over 11232\n",
      "batch 1630 over 11232\n",
      "batch 1640 over 11232\n",
      "batch 1650 over 11232\n",
      "batch 1660 over 11232\n",
      "batch 1670 over 11232\n",
      "batch 1680 over 11232\n",
      "batch 1690 over 11232\n",
      "batch 1700 over 11232\n",
      "batch 1710 over 11232\n",
      "batch 1720 over 11232\n",
      "batch 1730 over 11232\n",
      "batch 1740 over 11232\n",
      "batch 1750 over 11232\n",
      "batch 1760 over 11232\n",
      "batch 1770 over 11232\n",
      "batch 1780 over 11232\n",
      "batch 1790 over 11232\n",
      "batch 1800 over 11232\n",
      "batch 1810 over 11232\n",
      "batch 1820 over 11232\n",
      "batch 1830 over 11232\n",
      "batch 1840 over 11232\n",
      "batch 1850 over 11232\n",
      "batch 1860 over 11232\n",
      "batch 1870 over 11232\n",
      "batch 1880 over 11232\n",
      "batch 1890 over 11232\n",
      "batch 1900 over 11232\n",
      "batch 1910 over 11232\n",
      "batch 1920 over 11232\n",
      "batch 1930 over 11232\n",
      "batch 1940 over 11232\n",
      "batch 1950 over 11232\n",
      "batch 1960 over 11232\n",
      "batch 1970 over 11232\n",
      "batch 1980 over 11232\n",
      "batch 1990 over 11232\n",
      "batch 2000 over 11232\n",
      "batch 2010 over 11232\n",
      "batch 2020 over 11232\n",
      "batch 2030 over 11232\n",
      "batch 2040 over 11232\n",
      "batch 2050 over 11232\n",
      "batch 2060 over 11232\n",
      "batch 2070 over 11232\n",
      "batch 2080 over 11232\n",
      "batch 2090 over 11232\n",
      "batch 2100 over 11232\n",
      "batch 2110 over 11232\n",
      "batch 2120 over 11232\n",
      "batch 2130 over 11232\n",
      "batch 2140 over 11232\n",
      "batch 2150 over 11232\n",
      "batch 2160 over 11232\n",
      "batch 2170 over 11232\n",
      "batch 2180 over 11232\n",
      "batch 2190 over 11232\n",
      "batch 2200 over 11232\n",
      "batch 2210 over 11232\n",
      "batch 2220 over 11232\n",
      "batch 2230 over 11232\n",
      "batch 2240 over 11232\n",
      "batch 2250 over 11232\n",
      "batch 2260 over 11232\n",
      "batch 2270 over 11232\n",
      "batch 2280 over 11232\n",
      "batch 2290 over 11232\n",
      "batch 2300 over 11232\n",
      "batch 2310 over 11232\n",
      "batch 2320 over 11232\n",
      "batch 2330 over 11232\n",
      "batch 2340 over 11232\n",
      "batch 2350 over 11232\n",
      "batch 2360 over 11232\n",
      "batch 2370 over 11232\n",
      "batch 2380 over 11232\n",
      "batch 2390 over 11232\n",
      "batch 2400 over 11232\n",
      "batch 2410 over 11232\n",
      "batch 2420 over 11232\n",
      "batch 2430 over 11232\n",
      "batch 2440 over 11232\n",
      "batch 2450 over 11232\n",
      "batch 2460 over 11232\n",
      "batch 2470 over 11232\n",
      "batch 2480 over 11232\n",
      "batch 2490 over 11232\n",
      "batch 2500 over 11232\n",
      "batch 2510 over 11232\n",
      "batch 2520 over 11232\n",
      "batch 2530 over 11232\n",
      "batch 2540 over 11232\n",
      "batch 2550 over 11232\n",
      "batch 2560 over 11232\n",
      "batch 2570 over 11232\n",
      "batch 2580 over 11232\n",
      "batch 2590 over 11232\n",
      "batch 2600 over 11232\n",
      "batch 2610 over 11232\n",
      "batch 2620 over 11232\n",
      "batch 2630 over 11232\n",
      "batch 2640 over 11232\n",
      "batch 2650 over 11232\n",
      "batch 2660 over 11232\n",
      "batch 2670 over 11232\n",
      "batch 2680 over 11232\n",
      "batch 2690 over 11232\n",
      "batch 2700 over 11232\n",
      "batch 2710 over 11232\n",
      "batch 2720 over 11232\n",
      "batch 2730 over 11232\n",
      "batch 2740 over 11232\n",
      "batch 2750 over 11232\n",
      "batch 2760 over 11232\n",
      "batch 2770 over 11232\n",
      "batch 2780 over 11232\n",
      "batch 2790 over 11232\n",
      "batch 2800 over 11232\n",
      "batch 2810 over 11232\n",
      "batch 2820 over 11232\n",
      "batch 2830 over 11232\n",
      "batch 2840 over 11232\n",
      "batch 2850 over 11232\n",
      "batch 2860 over 11232\n",
      "batch 2870 over 11232\n",
      "batch 2880 over 11232\n",
      "batch 2890 over 11232\n",
      "batch 2900 over 11232\n",
      "batch 2910 over 11232\n",
      "batch 2920 over 11232\n",
      "batch 2930 over 11232\n",
      "batch 2940 over 11232\n",
      "batch 2950 over 11232\n",
      "batch 2960 over 11232\n",
      "batch 2970 over 11232\n",
      "batch 2980 over 11232\n",
      "batch 2990 over 11232\n",
      "batch 3000 over 11232\n",
      "batch 3010 over 11232\n",
      "batch 3020 over 11232\n",
      "batch 3030 over 11232\n",
      "batch 3040 over 11232\n",
      "batch 3050 over 11232\n",
      "batch 3060 over 11232\n",
      "batch 3070 over 11232\n",
      "batch 3080 over 11232\n",
      "batch 3090 over 11232\n",
      "batch 3100 over 11232\n",
      "batch 3110 over 11232\n",
      "batch 3120 over 11232\n",
      "batch 3130 over 11232\n",
      "batch 3140 over 11232\n",
      "batch 3150 over 11232\n",
      "batch 3160 over 11232\n",
      "batch 3170 over 11232\n",
      "batch 3180 over 11232\n",
      "batch 3190 over 11232\n",
      "batch 3200 over 11232\n",
      "batch 3210 over 11232\n",
      "batch 3220 over 11232\n",
      "batch 3230 over 11232\n",
      "batch 3240 over 11232\n",
      "batch 3250 over 11232\n",
      "batch 3260 over 11232\n",
      "batch 3270 over 11232\n",
      "batch 3280 over 11232\n",
      "batch 3290 over 11232\n",
      "batch 3300 over 11232\n",
      "batch 3310 over 11232\n",
      "batch 3320 over 11232\n",
      "batch 3330 over 11232\n",
      "batch 3340 over 11232\n",
      "batch 3350 over 11232\n",
      "batch 3360 over 11232\n",
      "batch 3370 over 11232\n",
      "batch 3380 over 11232\n",
      "batch 3390 over 11232\n",
      "batch 3400 over 11232\n",
      "batch 3410 over 11232\n",
      "batch 3420 over 11232\n",
      "batch 3430 over 11232\n",
      "batch 3440 over 11232\n",
      "batch 3450 over 11232\n",
      "batch 3460 over 11232\n",
      "batch 3470 over 11232\n",
      "batch 3480 over 11232\n",
      "batch 3490 over 11232\n",
      "batch 3500 over 11232\n",
      "batch 3510 over 11232\n",
      "batch 3520 over 11232\n",
      "batch 3530 over 11232\n",
      "batch 3540 over 11232\n",
      "batch 3550 over 11232\n",
      "batch 3560 over 11232\n",
      "batch 3570 over 11232\n",
      "batch 3580 over 11232\n",
      "batch 3590 over 11232\n",
      "batch 3600 over 11232\n",
      "batch 3610 over 11232\n",
      "batch 3620 over 11232\n",
      "batch 3630 over 11232\n",
      "batch 3640 over 11232\n",
      "batch 3650 over 11232\n",
      "batch 3660 over 11232\n",
      "batch 3670 over 11232\n",
      "batch 3680 over 11232\n",
      "batch 3690 over 11232\n",
      "batch 3700 over 11232\n",
      "batch 3710 over 11232\n",
      "batch 3720 over 11232\n",
      "batch 3730 over 11232\n",
      "batch 3740 over 11232\n",
      "batch 3750 over 11232\n",
      "batch 3760 over 11232\n",
      "batch 3770 over 11232\n",
      "batch 3780 over 11232\n",
      "batch 3790 over 11232\n",
      "batch 3800 over 11232\n",
      "batch 3810 over 11232\n",
      "batch 3820 over 11232\n",
      "batch 3830 over 11232\n",
      "batch 3840 over 11232\n",
      "batch 3850 over 11232\n",
      "batch 3860 over 11232\n",
      "batch 3870 over 11232\n",
      "batch 3880 over 11232\n",
      "batch 3890 over 11232\n",
      "batch 3900 over 11232\n",
      "batch 3910 over 11232\n",
      "batch 3920 over 11232\n",
      "batch 3930 over 11232\n",
      "batch 3940 over 11232\n",
      "batch 3950 over 11232\n",
      "batch 3960 over 11232\n",
      "batch 3970 over 11232\n",
      "batch 3980 over 11232\n",
      "batch 3990 over 11232\n",
      "batch 4000 over 11232\n",
      "batch 4010 over 11232\n",
      "batch 4020 over 11232\n",
      "batch 4030 over 11232\n",
      "batch 4040 over 11232\n",
      "batch 4050 over 11232\n",
      "batch 4060 over 11232\n",
      "batch 4070 over 11232\n",
      "batch 4080 over 11232\n",
      "batch 4090 over 11232\n",
      "batch 4100 over 11232\n",
      "batch 4110 over 11232\n",
      "batch 4120 over 11232\n",
      "batch 4130 over 11232\n",
      "batch 4140 over 11232\n",
      "batch 4150 over 11232\n",
      "batch 4160 over 11232\n",
      "batch 4170 over 11232\n",
      "batch 4180 over 11232\n",
      "batch 4190 over 11232\n",
      "batch 4200 over 11232\n",
      "batch 4210 over 11232\n",
      "batch 4220 over 11232\n",
      "batch 4230 over 11232\n",
      "batch 4240 over 11232\n",
      "batch 4250 over 11232\n",
      "batch 4260 over 11232\n",
      "batch 4270 over 11232\n",
      "batch 4280 over 11232\n",
      "batch 4290 over 11232\n",
      "batch 4300 over 11232\n",
      "batch 4310 over 11232\n",
      "batch 4320 over 11232\n",
      "batch 4330 over 11232\n",
      "batch 4340 over 11232\n",
      "batch 4350 over 11232\n",
      "batch 4360 over 11232\n",
      "batch 4370 over 11232\n",
      "batch 4380 over 11232\n",
      "batch 4390 over 11232\n",
      "batch 4400 over 11232\n",
      "batch 4410 over 11232\n",
      "batch 4420 over 11232\n",
      "batch 4430 over 11232\n",
      "batch 4440 over 11232\n",
      "batch 4450 over 11232\n",
      "batch 4460 over 11232\n",
      "batch 4470 over 11232\n",
      "batch 4480 over 11232\n",
      "batch 4490 over 11232\n",
      "batch 4500 over 11232\n",
      "batch 4510 over 11232\n",
      "batch 4520 over 11232\n",
      "batch 4530 over 11232\n",
      "batch 4540 over 11232\n",
      "batch 4550 over 11232\n",
      "batch 4560 over 11232\n",
      "batch 4570 over 11232\n",
      "batch 4580 over 11232\n",
      "batch 4590 over 11232\n",
      "batch 4600 over 11232\n",
      "batch 4610 over 11232\n",
      "batch 4620 over 11232\n",
      "batch 4630 over 11232\n",
      "batch 4640 over 11232\n",
      "batch 4650 over 11232\n",
      "batch 4660 over 11232\n",
      "batch 4670 over 11232\n",
      "batch 4680 over 11232\n",
      "batch 4690 over 11232\n",
      "batch 4700 over 11232\n",
      "batch 4710 over 11232\n",
      "batch 4720 over 11232\n",
      "batch 4730 over 11232\n",
      "batch 4740 over 11232\n",
      "batch 4750 over 11232\n",
      "batch 4760 over 11232\n",
      "batch 4770 over 11232\n",
      "batch 4780 over 11232\n",
      "batch 4790 over 11232\n",
      "batch 4800 over 11232\n",
      "batch 4810 over 11232\n",
      "batch 4820 over 11232\n",
      "batch 4830 over 11232\n",
      "batch 4840 over 11232\n",
      "batch 4850 over 11232\n",
      "batch 4860 over 11232\n",
      "batch 4870 over 11232\n",
      "batch 4880 over 11232\n",
      "batch 4890 over 11232\n",
      "batch 4900 over 11232\n",
      "batch 4910 over 11232\n",
      "batch 4920 over 11232\n",
      "batch 4930 over 11232\n",
      "batch 4940 over 11232\n",
      "batch 4950 over 11232\n",
      "batch 4960 over 11232\n",
      "batch 4970 over 11232\n",
      "batch 4980 over 11232\n",
      "batch 4990 over 11232\n",
      "batch 5000 over 11232\n",
      "batch 5010 over 11232\n",
      "batch 5020 over 11232\n",
      "batch 5030 over 11232\n",
      "batch 5040 over 11232\n",
      "batch 5050 over 11232\n",
      "batch 5060 over 11232\n",
      "batch 5070 over 11232\n",
      "batch 5080 over 11232\n",
      "batch 5090 over 11232\n",
      "batch 5100 over 11232\n",
      "batch 5110 over 11232\n",
      "batch 5120 over 11232\n",
      "batch 5130 over 11232\n",
      "batch 5140 over 11232\n",
      "batch 5150 over 11232\n",
      "batch 5160 over 11232\n",
      "batch 5170 over 11232\n",
      "batch 5180 over 11232\n",
      "batch 5190 over 11232\n",
      "batch 5200 over 11232\n",
      "batch 5210 over 11232\n",
      "batch 5220 over 11232\n",
      "batch 5230 over 11232\n",
      "batch 5240 over 11232\n",
      "batch 5250 over 11232\n",
      "batch 5260 over 11232\n",
      "batch 5270 over 11232\n",
      "batch 5280 over 11232\n",
      "batch 5290 over 11232\n",
      "batch 5300 over 11232\n",
      "batch 5310 over 11232\n",
      "batch 5320 over 11232\n",
      "batch 5330 over 11232\n",
      "batch 5340 over 11232\n",
      "batch 5350 over 11232\n",
      "batch 5360 over 11232\n",
      "batch 5370 over 11232\n",
      "batch 5380 over 11232\n",
      "batch 5390 over 11232\n",
      "batch 5400 over 11232\n",
      "batch 5410 over 11232\n",
      "batch 5420 over 11232\n",
      "batch 5430 over 11232\n",
      "batch 5440 over 11232\n",
      "batch 5450 over 11232\n",
      "batch 5460 over 11232\n",
      "batch 5470 over 11232\n",
      "batch 5480 over 11232\n",
      "batch 5490 over 11232\n",
      "batch 5500 over 11232\n",
      "batch 5510 over 11232\n",
      "batch 5520 over 11232\n",
      "batch 5530 over 11232\n",
      "batch 5540 over 11232\n",
      "batch 5550 over 11232\n",
      "batch 5560 over 11232\n",
      "batch 5570 over 11232\n",
      "batch 5580 over 11232\n",
      "batch 5590 over 11232\n",
      "batch 5600 over 11232\n",
      "batch 5610 over 11232\n",
      "batch 5620 over 11232\n",
      "batch 5630 over 11232\n",
      "batch 5640 over 11232\n",
      "batch 5650 over 11232\n",
      "batch 5660 over 11232\n",
      "batch 5670 over 11232\n",
      "batch 5680 over 11232\n",
      "batch 5690 over 11232\n",
      "batch 5700 over 11232\n",
      "batch 5710 over 11232\n",
      "batch 5720 over 11232\n",
      "batch 5730 over 11232\n",
      "batch 5740 over 11232\n",
      "batch 5750 over 11232\n",
      "batch 5760 over 11232\n",
      "batch 5770 over 11232\n",
      "batch 5780 over 11232\n",
      "batch 5790 over 11232\n",
      "batch 5800 over 11232\n",
      "batch 5810 over 11232\n",
      "batch 5820 over 11232\n",
      "batch 5830 over 11232\n",
      "batch 5840 over 11232\n",
      "batch 5850 over 11232\n",
      "batch 5860 over 11232\n",
      "batch 5870 over 11232\n",
      "batch 5880 over 11232\n",
      "batch 5890 over 11232\n",
      "batch 5900 over 11232\n",
      "batch 5910 over 11232\n",
      "batch 5920 over 11232\n",
      "batch 5930 over 11232\n",
      "batch 5940 over 11232\n",
      "batch 5950 over 11232\n",
      "batch 5960 over 11232\n",
      "batch 5970 over 11232\n",
      "batch 5980 over 11232\n",
      "batch 5990 over 11232\n",
      "batch 6000 over 11232\n",
      "batch 6010 over 11232\n",
      "batch 6020 over 11232\n",
      "batch 6030 over 11232\n",
      "batch 6040 over 11232\n",
      "batch 6050 over 11232\n",
      "batch 6060 over 11232\n",
      "batch 6070 over 11232\n",
      "batch 6080 over 11232\n",
      "batch 6090 over 11232\n",
      "batch 6100 over 11232\n",
      "batch 6110 over 11232\n",
      "batch 6120 over 11232\n",
      "batch 6130 over 11232\n",
      "batch 6140 over 11232\n",
      "batch 6150 over 11232\n",
      "batch 6160 over 11232\n",
      "batch 6170 over 11232\n",
      "batch 6180 over 11232\n",
      "batch 6190 over 11232\n",
      "batch 6200 over 11232\n",
      "batch 6210 over 11232\n",
      "batch 6220 over 11232\n",
      "batch 6230 over 11232\n",
      "batch 6240 over 11232\n",
      "batch 6250 over 11232\n",
      "batch 6260 over 11232\n",
      "batch 6270 over 11232\n",
      "batch 6280 over 11232\n",
      "batch 6290 over 11232\n",
      "batch 6300 over 11232\n",
      "batch 6310 over 11232\n",
      "batch 6320 over 11232\n",
      "batch 6330 over 11232\n",
      "batch 6340 over 11232\n",
      "batch 6350 over 11232\n",
      "batch 6360 over 11232\n",
      "batch 6370 over 11232\n",
      "batch 6380 over 11232\n",
      "batch 6390 over 11232\n",
      "batch 6400 over 11232\n",
      "batch 6410 over 11232\n",
      "batch 6420 over 11232\n",
      "batch 6430 over 11232\n",
      "batch 6440 over 11232\n",
      "batch 6450 over 11232\n",
      "batch 6460 over 11232\n",
      "batch 6470 over 11232\n",
      "batch 6480 over 11232\n",
      "batch 6490 over 11232\n",
      "batch 6500 over 11232\n",
      "batch 6510 over 11232\n",
      "batch 6520 over 11232\n",
      "batch 6530 over 11232\n",
      "batch 6540 over 11232\n",
      "batch 6550 over 11232\n",
      "batch 6560 over 11232\n",
      "batch 6570 over 11232\n",
      "batch 6580 over 11232\n",
      "batch 6590 over 11232\n",
      "batch 6600 over 11232\n",
      "batch 6610 over 11232\n",
      "batch 6620 over 11232\n",
      "batch 6630 over 11232\n",
      "batch 6640 over 11232\n",
      "batch 6650 over 11232\n",
      "batch 6660 over 11232\n",
      "batch 6670 over 11232\n",
      "batch 6680 over 11232\n",
      "batch 6690 over 11232\n",
      "batch 6700 over 11232\n",
      "batch 6710 over 11232\n",
      "batch 6720 over 11232\n",
      "batch 6730 over 11232\n",
      "batch 6740 over 11232\n",
      "batch 6750 over 11232\n",
      "batch 6760 over 11232\n",
      "batch 6770 over 11232\n",
      "batch 6780 over 11232\n",
      "batch 6790 over 11232\n",
      "batch 6800 over 11232\n",
      "batch 6810 over 11232\n",
      "batch 6820 over 11232\n",
      "batch 6830 over 11232\n",
      "batch 6840 over 11232\n",
      "batch 6850 over 11232\n",
      "batch 6860 over 11232\n",
      "batch 6870 over 11232\n",
      "batch 6880 over 11232\n",
      "batch 6890 over 11232\n",
      "batch 6900 over 11232\n",
      "batch 6910 over 11232\n",
      "batch 6920 over 11232\n",
      "batch 6930 over 11232\n",
      "batch 6940 over 11232\n",
      "batch 6950 over 11232\n",
      "batch 6960 over 11232\n",
      "batch 6970 over 11232\n",
      "batch 6980 over 11232\n",
      "batch 6990 over 11232\n",
      "batch 7000 over 11232\n",
      "batch 7010 over 11232\n",
      "batch 7020 over 11232\n",
      "batch 7030 over 11232\n",
      "batch 7040 over 11232\n",
      "batch 7050 over 11232\n",
      "batch 7060 over 11232\n",
      "batch 7070 over 11232\n",
      "batch 7080 over 11232\n",
      "batch 7090 over 11232\n",
      "batch 7100 over 11232\n",
      "batch 7110 over 11232\n",
      "batch 7120 over 11232\n",
      "batch 7130 over 11232\n",
      "batch 7140 over 11232\n",
      "batch 7150 over 11232\n",
      "batch 7160 over 11232\n",
      "batch 7170 over 11232\n",
      "batch 7180 over 11232\n",
      "batch 7190 over 11232\n",
      "batch 7200 over 11232\n",
      "batch 7210 over 11232\n",
      "batch 7220 over 11232\n",
      "batch 7230 over 11232\n",
      "batch 7240 over 11232\n",
      "batch 7250 over 11232\n",
      "batch 7260 over 11232\n",
      "batch 7270 over 11232\n",
      "batch 7280 over 11232\n",
      "batch 7290 over 11232\n",
      "batch 7300 over 11232\n",
      "batch 7310 over 11232\n",
      "batch 7320 over 11232\n",
      "batch 7330 over 11232\n",
      "batch 7340 over 11232\n",
      "batch 7350 over 11232\n",
      "batch 7360 over 11232\n",
      "batch 7370 over 11232\n",
      "batch 7380 over 11232\n",
      "batch 7390 over 11232\n",
      "batch 7400 over 11232\n",
      "batch 7410 over 11232\n",
      "batch 7420 over 11232\n",
      "batch 7430 over 11232\n",
      "batch 7440 over 11232\n",
      "batch 7450 over 11232\n",
      "batch 7460 over 11232\n",
      "batch 7470 over 11232\n",
      "batch 7480 over 11232\n",
      "batch 7490 over 11232\n",
      "batch 7500 over 11232\n",
      "batch 7510 over 11232\n",
      "batch 7520 over 11232\n",
      "batch 7530 over 11232\n",
      "batch 7540 over 11232\n",
      "batch 7550 over 11232\n",
      "batch 7560 over 11232\n",
      "batch 7570 over 11232\n",
      "batch 7580 over 11232\n",
      "batch 7590 over 11232\n",
      "batch 7600 over 11232\n",
      "batch 7610 over 11232\n",
      "batch 7620 over 11232\n",
      "batch 7630 over 11232\n",
      "batch 7640 over 11232\n",
      "batch 7650 over 11232\n",
      "batch 7660 over 11232\n",
      "batch 7670 over 11232\n",
      "batch 7680 over 11232\n",
      "batch 7690 over 11232\n",
      "batch 7700 over 11232\n",
      "batch 7710 over 11232\n",
      "batch 7720 over 11232\n",
      "batch 7730 over 11232\n",
      "batch 7740 over 11232\n",
      "batch 7750 over 11232\n",
      "batch 7760 over 11232\n",
      "batch 7770 over 11232\n",
      "batch 7780 over 11232\n",
      "batch 7790 over 11232\n",
      "batch 7800 over 11232\n",
      "batch 7810 over 11232\n",
      "batch 7820 over 11232\n",
      "batch 7830 over 11232\n",
      "batch 7840 over 11232\n",
      "batch 7850 over 11232\n",
      "batch 7860 over 11232\n",
      "batch 7870 over 11232\n",
      "batch 7880 over 11232\n",
      "batch 7890 over 11232\n",
      "batch 7900 over 11232\n",
      "batch 7910 over 11232\n",
      "batch 7920 over 11232\n",
      "batch 7930 over 11232\n",
      "batch 7940 over 11232\n",
      "batch 7950 over 11232\n",
      "batch 7960 over 11232\n",
      "batch 7970 over 11232\n",
      "batch 7980 over 11232\n",
      "batch 7990 over 11232\n",
      "batch 8000 over 11232\n",
      "batch 8010 over 11232\n",
      "batch 8020 over 11232\n",
      "batch 8030 over 11232\n",
      "batch 8040 over 11232\n",
      "batch 8050 over 11232\n",
      "batch 8060 over 11232\n",
      "batch 8070 over 11232\n",
      "batch 8080 over 11232\n",
      "batch 8090 over 11232\n",
      "batch 8100 over 11232\n",
      "batch 8110 over 11232\n",
      "batch 8120 over 11232\n",
      "batch 8130 over 11232\n",
      "batch 8140 over 11232\n",
      "batch 8150 over 11232\n",
      "batch 8160 over 11232\n",
      "batch 8170 over 11232\n",
      "batch 8180 over 11232\n",
      "batch 8190 over 11232\n",
      "batch 8200 over 11232\n",
      "batch 8210 over 11232\n",
      "batch 8220 over 11232\n",
      "batch 8230 over 11232\n",
      "batch 8240 over 11232\n",
      "batch 8250 over 11232\n",
      "batch 8260 over 11232\n",
      "batch 8270 over 11232\n",
      "batch 8280 over 11232\n",
      "batch 8290 over 11232\n",
      "batch 8300 over 11232\n",
      "batch 8310 over 11232\n",
      "batch 8320 over 11232\n",
      "batch 8330 over 11232\n",
      "batch 8340 over 11232\n",
      "batch 8350 over 11232\n",
      "batch 8360 over 11232\n",
      "batch 8370 over 11232\n",
      "batch 8380 over 11232\n",
      "batch 8390 over 11232\n",
      "batch 8400 over 11232\n",
      "batch 8410 over 11232\n",
      "batch 8420 over 11232\n",
      "batch 8430 over 11232\n",
      "batch 8440 over 11232\n",
      "batch 8450 over 11232\n",
      "batch 8460 over 11232\n",
      "batch 8470 over 11232\n",
      "batch 8480 over 11232\n",
      "batch 8490 over 11232\n",
      "batch 8500 over 11232\n",
      "batch 8510 over 11232\n",
      "batch 8520 over 11232\n",
      "batch 8530 over 11232\n",
      "batch 8540 over 11232\n",
      "batch 8550 over 11232\n",
      "batch 8560 over 11232\n",
      "batch 8570 over 11232\n",
      "batch 8580 over 11232\n",
      "batch 8590 over 11232\n",
      "batch 8600 over 11232\n",
      "batch 8610 over 11232\n",
      "batch 8620 over 11232\n",
      "batch 8630 over 11232\n",
      "batch 8640 over 11232\n",
      "batch 8650 over 11232\n",
      "batch 8660 over 11232\n",
      "batch 8670 over 11232\n",
      "batch 8680 over 11232\n",
      "batch 8690 over 11232\n",
      "batch 8700 over 11232\n",
      "batch 8710 over 11232\n",
      "batch 8720 over 11232\n",
      "batch 8730 over 11232\n",
      "batch 8740 over 11232\n",
      "batch 8750 over 11232\n",
      "batch 8760 over 11232\n",
      "batch 8770 over 11232\n",
      "batch 8780 over 11232\n",
      "batch 8790 over 11232\n",
      "batch 8800 over 11232\n",
      "batch 8810 over 11232\n",
      "batch 8820 over 11232\n",
      "batch 8830 over 11232\n",
      "batch 8840 over 11232\n",
      "batch 8850 over 11232\n",
      "batch 8860 over 11232\n",
      "batch 8870 over 11232\n",
      "batch 8880 over 11232\n",
      "batch 8890 over 11232\n",
      "batch 8900 over 11232\n",
      "batch 8910 over 11232\n",
      "batch 8920 over 11232\n",
      "batch 8930 over 11232\n",
      "batch 8940 over 11232\n",
      "batch 8950 over 11232\n",
      "batch 8960 over 11232\n",
      "batch 8970 over 11232\n",
      "batch 8980 over 11232\n",
      "batch 8990 over 11232\n",
      "batch 9000 over 11232\n",
      "batch 9010 over 11232\n",
      "batch 9020 over 11232\n",
      "batch 9030 over 11232\n",
      "batch 9040 over 11232\n",
      "batch 9050 over 11232\n",
      "batch 9060 over 11232\n",
      "batch 9070 over 11232\n",
      "batch 9080 over 11232\n",
      "batch 9090 over 11232\n",
      "batch 9100 over 11232\n",
      "batch 9110 over 11232\n",
      "batch 9120 over 11232\n",
      "batch 9130 over 11232\n",
      "batch 9140 over 11232\n",
      "batch 9150 over 11232\n",
      "batch 9160 over 11232\n",
      "batch 9170 over 11232\n",
      "batch 9180 over 11232\n",
      "batch 9190 over 11232\n",
      "batch 9200 over 11232\n",
      "batch 9210 over 11232\n",
      "batch 9220 over 11232\n",
      "batch 9230 over 11232\n",
      "batch 9240 over 11232\n",
      "batch 9250 over 11232\n",
      "batch 9260 over 11232\n",
      "batch 9270 over 11232\n",
      "batch 9280 over 11232\n",
      "batch 9290 over 11232\n",
      "batch 9300 over 11232\n",
      "batch 9310 over 11232\n",
      "batch 9320 over 11232\n",
      "batch 9330 over 11232\n",
      "batch 9340 over 11232\n",
      "batch 9350 over 11232\n",
      "batch 9360 over 11232\n",
      "batch 9370 over 11232\n",
      "batch 9380 over 11232\n",
      "batch 9390 over 11232\n",
      "batch 9400 over 11232\n",
      "batch 9410 over 11232\n",
      "batch 9420 over 11232\n",
      "batch 9430 over 11232\n",
      "batch 9440 over 11232\n",
      "batch 9450 over 11232\n",
      "batch 9460 over 11232\n",
      "batch 9470 over 11232\n",
      "batch 9480 over 11232\n",
      "batch 9490 over 11232\n",
      "batch 9500 over 11232\n",
      "batch 9510 over 11232\n",
      "batch 9520 over 11232\n",
      "batch 9530 over 11232\n",
      "batch 9540 over 11232\n",
      "batch 9550 over 11232\n",
      "batch 9560 over 11232\n",
      "batch 9570 over 11232\n",
      "batch 9580 over 11232\n",
      "batch 9590 over 11232\n",
      "batch 9600 over 11232\n",
      "batch 9610 over 11232\n",
      "batch 9620 over 11232\n",
      "batch 9630 over 11232\n",
      "batch 9640 over 11232\n",
      "batch 9650 over 11232\n",
      "batch 9660 over 11232\n",
      "batch 9670 over 11232\n",
      "batch 9680 over 11232\n",
      "batch 9690 over 11232\n",
      "batch 9700 over 11232\n",
      "batch 9710 over 11232\n",
      "batch 9720 over 11232\n",
      "batch 9730 over 11232\n",
      "batch 9740 over 11232\n",
      "batch 9750 over 11232\n",
      "batch 9760 over 11232\n",
      "batch 9770 over 11232\n",
      "batch 9780 over 11232\n",
      "batch 9790 over 11232\n",
      "batch 9800 over 11232\n",
      "batch 9810 over 11232\n",
      "batch 9820 over 11232\n",
      "batch 9830 over 11232\n",
      "batch 9840 over 11232\n",
      "batch 9850 over 11232\n",
      "batch 9860 over 11232\n",
      "batch 9870 over 11232\n",
      "batch 9880 over 11232\n",
      "batch 9890 over 11232\n",
      "batch 9900 over 11232\n",
      "batch 9910 over 11232\n",
      "batch 9920 over 11232\n",
      "batch 9930 over 11232\n",
      "batch 9940 over 11232\n",
      "batch 9950 over 11232\n",
      "batch 9960 over 11232\n",
      "batch 9970 over 11232\n",
      "batch 9980 over 11232\n",
      "batch 9990 over 11232\n",
      "batch 10000 over 11232\n",
      "batch 10010 over 11232\n",
      "batch 10020 over 11232\n",
      "batch 10030 over 11232\n",
      "batch 10040 over 11232\n",
      "batch 10050 over 11232\n",
      "batch 10060 over 11232\n",
      "batch 10070 over 11232\n",
      "batch 10080 over 11232\n",
      "batch 10090 over 11232\n",
      "batch 10100 over 11232\n",
      "batch 10110 over 11232\n",
      "batch 10120 over 11232\n",
      "batch 10130 over 11232\n",
      "batch 10140 over 11232\n",
      "batch 10150 over 11232\n",
      "batch 10160 over 11232\n",
      "batch 10170 over 11232\n",
      "batch 10180 over 11232\n",
      "batch 10190 over 11232\n",
      "batch 10200 over 11232\n",
      "batch 10210 over 11232\n",
      "batch 10220 over 11232\n",
      "batch 10230 over 11232\n",
      "batch 10240 over 11232\n",
      "batch 10250 over 11232\n",
      "batch 10260 over 11232\n",
      "batch 10270 over 11232\n",
      "batch 10280 over 11232\n",
      "batch 10290 over 11232\n",
      "batch 10300 over 11232\n",
      "batch 10310 over 11232\n",
      "batch 10320 over 11232\n",
      "batch 10330 over 11232\n",
      "batch 10340 over 11232\n",
      "batch 10350 over 11232\n",
      "batch 10360 over 11232\n",
      "batch 10370 over 11232\n",
      "batch 10380 over 11232\n",
      "batch 10390 over 11232\n",
      "batch 10400 over 11232\n",
      "batch 10410 over 11232\n",
      "batch 10420 over 11232\n",
      "batch 10430 over 11232\n",
      "batch 10440 over 11232\n",
      "batch 10450 over 11232\n",
      "batch 10460 over 11232\n",
      "batch 10470 over 11232\n",
      "batch 10480 over 11232\n",
      "batch 10490 over 11232\n",
      "batch 10500 over 11232\n",
      "batch 10510 over 11232\n",
      "batch 10520 over 11232\n",
      "batch 10530 over 11232\n",
      "batch 10540 over 11232\n",
      "batch 10550 over 11232\n",
      "batch 10560 over 11232\n",
      "batch 10570 over 11232\n",
      "batch 10580 over 11232\n",
      "batch 10590 over 11232\n",
      "batch 10600 over 11232\n",
      "batch 10610 over 11232\n",
      "batch 10620 over 11232\n",
      "batch 10630 over 11232\n",
      "batch 10640 over 11232\n",
      "batch 10650 over 11232\n",
      "batch 10660 over 11232\n",
      "batch 10670 over 11232\n",
      "batch 10680 over 11232\n",
      "batch 10690 over 11232\n",
      "batch 10700 over 11232\n",
      "batch 10710 over 11232\n",
      "batch 10720 over 11232\n",
      "batch 10730 over 11232\n",
      "batch 10740 over 11232\n",
      "batch 10750 over 11232\n",
      "batch 10760 over 11232\n",
      "batch 10770 over 11232\n",
      "batch 10780 over 11232\n",
      "batch 10790 over 11232\n",
      "batch 10800 over 11232\n",
      "batch 10810 over 11232\n",
      "batch 10820 over 11232\n",
      "batch 10830 over 11232\n",
      "batch 10840 over 11232\n",
      "batch 10850 over 11232\n",
      "batch 10860 over 11232\n",
      "batch 10870 over 11232\n",
      "batch 10880 over 11232\n",
      "batch 10890 over 11232\n",
      "batch 10900 over 11232\n",
      "batch 10910 over 11232\n",
      "batch 10920 over 11232\n",
      "batch 10930 over 11232\n",
      "batch 10940 over 11232\n",
      "batch 10950 over 11232\n",
      "batch 10960 over 11232\n",
      "batch 10970 over 11232\n",
      "batch 10980 over 11232\n",
      "batch 10990 over 11232\n",
      "batch 11000 over 11232\n",
      "batch 11010 over 11232\n",
      "batch 11020 over 11232\n",
      "batch 11030 over 11232\n",
      "batch 11040 over 11232\n",
      "batch 11050 over 11232\n",
      "batch 11060 over 11232\n",
      "batch 11070 over 11232\n",
      "batch 11080 over 11232\n",
      "batch 11090 over 11232\n",
      "batch 11100 over 11232\n",
      "batch 11110 over 11232\n",
      "batch 11120 over 11232\n",
      "batch 11130 over 11232\n",
      "batch 11140 over 11232\n",
      "batch 11150 over 11232\n",
      "batch 11160 over 11232\n",
      "batch 11170 over 11232\n",
      "batch 11180 over 11232\n",
      "batch 11190 over 11232\n",
      "batch 11200 over 11232\n",
      "batch 11210 over 11232\n",
      "batch 11220 over 11232\n",
      "batch 11230 over 11232\n"
     ]
    }
   ],
   "source": [
    "activation = nn.Softmax(dim=1)\n",
    "predictions = []\n",
    "pair_ids_list = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for batch in data_loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_pair_ids = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = activation(logits)\n",
    "    tmp_logits = torch.max(logits, dim=1)\n",
    "    pred = tmp_logits[1]\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    predictions.append(pred)\n",
    "    pair_ids_list = pair_ids_list + b_pair_ids.tolist()\n",
    "    count += 1\n",
    "    if count % 10 == 0:\n",
    "        print('batch {} over {}'.format(count, len(data_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_flat = [item for sublist in predictions for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./Results/predictions_5_18.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions_flat, f)\n",
    "\n",
    "with open('./Results/pair_ids_5_18.pkl', 'wb') as f:\n",
    "    pickle.dump(pair_ids_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179700\n",
      "179700\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions_flat))\n",
    "print(len(pair_ids_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "f = open('./submissions/submission_05_18.csv','w', newline='')\n",
    "wr = csv.writer(f)\n",
    "wr.writerow(['pair_id','similar'])\n",
    "for i in range(len(pair_ids_list)):\n",
    "     wr.writerow([pair_ids_list[i],predictions_flat[i]])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5727fef1564fc40da7df37d47a0e0c2c896fcd54b1b9904c6fabf8b4c6e4eac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('huggingface': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
